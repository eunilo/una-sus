#!/usr/bin/env python3
"""
üß† Scraper UNA-SUS - Vers√£o Grounded Theory
===========================================

Esta √© uma vers√£o modific√°vel do scraper_unasus_melhorado.py criada especificamente
para aplica√ß√£o da metodologia Grounded Theory em pesquisas educacionais.

üéØ PROP√ìSITO:
- Vers√£o modific√°vel para pesquisa qualitativa
- Adapte crit√©rios conforme necessidades da sua pesquisa
- Processo iterativo de coleta e an√°lise
- Sistema de backup seguro

üìã COMO USAR:
1. Modifique os descritores DEIA conforme sua pesquisa
2. Ajuste campos coletados se necess√°rio
3. Execute o scraper e analise os resultados
4. Refine crit√©rios baseado nos insights
5. Repita at√© satura√ß√£o te√≥rica

üíæ BACKUP:
- Mantenha o arquivo scraper_unasus_backup_original.py intacto
- Use como ponto de retorno se necess√°rio

üî¨ METODOLOGIA:
- Grounded Theory: desenvolvimento de teorias a partir dos dados
- Coleta e an√°lise simult√¢neas
- Compara√ß√£o constante entre dados
- Satura√ß√£o te√≥rica como crit√©rio de parada

üìö Para mais informa√ß√µes, consulte:
- README_Grounded_Theory.md
- Documenta√ß√£o principal do projeto
"""

import json
import logging
import os
import re
import time
from datetime import datetime
from typing import Dict, List, Set

import pandas as pd
import requests
from bs4 import BeautifulSoup

# Configura√ß√µes da API
URL = "https://www.unasus.gov.br/cursos/rest/busca"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "X-Requested-With": "XMLHttpRequest",
    "Origin": "https://www.unasus.gov.br",
    "Referer": "https://www.unasus.gov.br/cursos/busca?status=todos&busca=&ordenacao=Relev%C3%A2ncia%20na%20busca",
}
COOKIES = {
    "PORTAL_UNASUS": "4ru34cs848mfbopb6vseqluni4",
    "UNASUSAnonID": "ID1ef7d6246158f7cf31c06b928bc56f8e",
    "_shibsession_64656661756c7468747470733a2f2f7777772e756e617375732e676f762e6272": "_329a72cffc11d2904ae393c82d0cfb72",
}
PAYLOAD_INICIAL = {
    "busca": "",
    "ordenacao": "Por nome",
    "status": "Todos",
    "proximo": 0,
}

# Descritores DEIA expandidos
DESCRITORES_DEIA = [
    # Descritores originais
    "Diversidade, Equidade e Integra√ß√£o",
    "Diversidade, Equidade, Inclus√£o e Pertencimento",
    "Diversidade, Equidade, Inclus√£o, Acessibilidade",
    "Diversidade, Equidade, Inclus√£o, Pertencimento",
    "Diversidade, Igualdade e Inclus√£o",
    "Diversidade, Igualdade, Inclus√£o e Acessibilidade",
    "Diversidade, Igualdade, Inclus√£o, Pertencimento",
    "Equidade, Diversidade e Inclus√£o",
    "Inclus√£o, Diversidade, Equidade e Acessibilidade",
    "Inclus√£o, Diversidade, Equidade, Acessibilidade",
    # Novos descritores e varia√ß√µes
    "Diversidade e Inclus√£o",
    "Equidade e Inclus√£o",
    "Diversidade, Equidade e Inclus√£o",
    "Inclus√£o e Diversidade",
    "Equidade e Diversidade",
    "Acessibilidade e Inclus√£o",
    "Diversidade, Equidade, Inclus√£o e Acessibilidade",
    "Inclus√£o, Equidade e Diversidade",
    "Diversidade, Inclus√£o e Equidade",
    "Equidade, Inclus√£o e Diversidade",
    # Termos individuais relacionados
    "Diversidade",
    "Equidade",
    "Inclus√£o",
    "Acessibilidade",
    "Pertencimento",
    "Inclusivo",
    "Inclusiva",
    "Diverso",
    "Diversa",
    "Equitativo",
    "Equitativa",
    "Acess√≠vel",
    # Termos espec√≠ficos de sa√∫de
    "Sa√∫de Mental",
    "Sa√∫de da Popula√ß√£o Negra",
    "Sa√∫de Ind√≠gena",
    "Sa√∫de LGBTQI+",
    "Sa√∫de da Mulher",
    "Sa√∫de do Idoso",
    "Sa√∫de da Crian√ßa",
    "Sa√∫de do Adolescente",
    "Sa√∫de da Pessoa com Defici√™ncia",
    "Sa√∫de da Popula√ß√£o em Situa√ß√£o de Rua",
    "Sa√∫de da Popula√ß√£o Privada de Liberdade",
    "Sa√∫de da Popula√ß√£o do Campo",
    "Sa√∫de da Popula√ß√£o da Floresta",
    "Sa√∫de da Popula√ß√£o das √Åguas",
    # Termos de vulnerabilidade social
    "Vulnerabilidade Social",
    "Popula√ß√£o Vulner√°vel",
    "Grupos Vulner√°veis",
    "Popula√ß√£o em Situa√ß√£o de Rua",
    "Popula√ß√£o Privada de Liberdade",
    "Popula√ß√£o do Campo",
    "Popula√ß√£o da Floresta",
    "Popula√ß√£o das √Åguas",
    "Popula√ß√£o Negra",
    "Popula√ß√£o Ind√≠gena",
    "Popula√ß√£o LGBTQI+",
    "Pessoa com Defici√™ncia",
    "Pessoas com Defici√™ncia",
    "Idoso",
    "Idosos",
    "Crian√ßa",
    "Crian√ßas",
    "Adolescente",
    "Adolescentes",
    "Mulher",
    "Mulheres",
    "Homem",
    "Homens",
    "Trans",
    "Transg√™nero",
    "Transg√™nera",
    "N√£o-bin√°rio",
    "N√£o-bin√°ria",
    "Gay",
    "L√©sbica",
    "Bissexual",
    "Pansexual",
    "Assexual",
    "Queer",
    "Intersexo",
    "Intersexual",
    "Negro",
    "Negra",
    "Negros",
    "Negras",
    "Ind√≠gena",
    "Ind√≠genas",
    "Quilombola",
    "Quilombolas",
    "Ribeirinho",
    "Ribeirinhos",
    "Extrativista",
    "Extrativistas",
    "Pescador",
    "Pescadores",
    "Agricultor",
    "Agricultores",
    "Sem-terra",
    "Sem-terras",
    "Sem-teto",
    "Sem-tetos",
    "Refugiado",
    "Refugiada",
    "Refugiados",
    "Refugiadas",
    "Imigrante",
    "Imigrantes",
    "Migrantes",
    "Migrantes",
    "Deslocado",
    "Deslocada",
    "Deslocados",
    "Deslocadas",
    "V√≠tima de Viol√™ncia",
    "V√≠timas de Viol√™ncia",
    "Sobrevivente de Viol√™ncia",
    "Sobreviventes de Viol√™ncia",
    "V√≠tima de Tr√°fico",
    "V√≠timas de Tr√°fico",
    "V√≠tima de Explora√ß√£o",
    "V√≠timas de Explora√ß√£o",
    "V√≠tima de Abuso",
    "V√≠timas de Abuso",
    "V√≠tima de Discrimina√ß√£o",
    "V√≠timas de Discrimina√ß√£o",
    "V√≠tima de Preconceito",
    "V√≠timas de Preconceito",
    "V√≠tima de Racismo",
    "V√≠timas de Racismo",
    "V√≠tima de Sexismo",
    "V√≠timas de Sexismo",
    "V√≠tima de Homofobia",
    "V√≠timas de Homofobia",
    "V√≠tima de Transfobia",
    "V√≠timas de Transfobia",
    "V√≠tima de Lesbofobia",
    "V√≠timas de Lesbofobia",
    "V√≠tima de Bifobia",
    "V√≠timas de Bifobia",
    "V√≠tima de Gordofobia",
    "V√≠timas de Gordofobia",
    "V√≠tima de Etarismo",
    "V√≠timas de Etarismo",
    "V√≠tima de Capacitismo",
    "V√≠timas de Capacitismo",
    "V√≠tima de Classismo",
    "V√≠timas de Classismo",
    "V√≠tima de Xenofobia",
    "V√≠timas de Xenofobia",
    "V√≠tima de Antissemitismo",
    "V√≠timas de Antissemitismo",
    "V√≠tima de Islamofobia",
    "V√≠timas de Islamofobia",
    "V√≠tima de Cristofobia",
    "V√≠timas de Cristofobia",
    "V√≠tima de Ateofobia",
    "V√≠timas de Ateofobia",
    "V√≠tima de Misoginia",
    "V√≠timas de Misoginia",
    "V√≠tima de Misandria",
    "V√≠timas de Misandria",
    "V√≠tima de Misantropia",
    "V√≠timas de Misantropia",
    "V√≠tima de Misoginia",
    "V√≠timas de Misoginia",
    "V√≠tima de Misandria",
    "V√≠timas de Misandria",
    "V√≠tima de Misantropia",
    "V√≠timas de Misantropia",
]


def setup_logging():
    """Configura o sistema de logging."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler("scraper_unasus_melhorado.log", encoding="utf-8"),
            logging.StreamHandler(),
        ],
    )
    return logging.getLogger(__name__)


def encontrar_descritor_deia_melhorado(texto_completo: str) -> str:
    """Busca descritores DEIA de forma mais abrangente."""
    if not texto_completo:
        return ""

    texto_lower = texto_completo.lower()
    descritores_encontrados = []

    for descritor in DESCRITORES_DEIA:
        if descritor.lower() in texto_lower:
            descritores_encontrados.append(descritor)

    # Retorna o descritor mais espec√≠fico (mais longo) encontrado
    if descritores_encontrados:
        return max(descritores_encontrados, key=len)

    return ""


def extrair_texto_pagina_inicial(id_curso: str, logger: logging.Logger) -> str:
    """Extrai o texto completo da p√°gina inicial do curso."""
    url_curso = f"https://www.unasus.gov.br/cursos/curso/{id_curso}"

    try:
        response = requests.get(url_curso, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.text, "html.parser")

        # Remove scripts e estilos
        for script in soup(["script", "style"]):
            script.decompose()

        # Extrai todo o texto da p√°gina
        texto_completo = soup.get_text()

        # Limpa o texto
        linhas = (linha.strip() for linha in texto_completo.splitlines())
        chunks = (frase.strip() for linha in linhas for frase in linha.split("  "))
        texto_limpo = " ".join(chunk for chunk in chunks if chunk)

        logger.debug(
            f"Texto da p√°gina inicial extra√≠do para curso {id_curso}: {len(texto_limpo)} caracteres"
        )
        return texto_limpo

    except Exception as e:
        logger.error(
            f"Erro ao extrair texto da p√°gina inicial do curso {id_curso}: {e}"
        )
        return ""


def extrair_descricao_curso_melhorada(id_curso: str, logger: logging.Logger) -> str:
    """Extrai a descri√ß√£o do curso de forma mais robusta."""
    url_curso = f"https://www.unasus.gov.br/cursos/curso/{id_curso}"

    try:
        response = requests.get(url_curso, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.text, "html.parser")

        # M√∫ltiplas estrat√©gias para encontrar a descri√ß√£o
        descricao = ""

        # 1. Buscar por seletores espec√≠ficos
        selectores = [
            "div.descricao",
            "div.curso-descricao",
            "p.descricao",
            "div.conteudo",
            "div.curso-info",
            "section.descricao",
            ".descricao-curso",
            "div.curso-detalhes",
            "div.informacoes-curso",
            "div.sobre-curso",
            "div.apresentacao",
            "div.introducao",
        ]

        for seletor in selectores:
            elemento = soup.select_one(seletor)
            if elemento:
                descricao = elemento.get_text(strip=True)
                if descricao and len(descricao) > 50:
                    logger.debug(f"Descri√ß√£o encontrada com seletor: {seletor}")
                    break

        # 2. Buscar por texto espec√≠fico
        if not descricao:
            elementos = soup.find_all(
                string=lambda t: t
                and any(
                    palavra in t.lower()
                    for palavra in [
                        "descri√ß√£o",
                        "sobre o curso",
                        "apresenta√ß√£o",
                        "introdu√ß√£o",
                        "objetivo",
                        "objetivos",
                        "p√∫blico-alvo",
                        "p√∫blico alvo",
                    ]
                )
            )

            for elemento in elementos:
                parent = elemento.parent
                if parent:
                    texto = parent.get_text(strip=True)
                    if len(texto) > 100:  # Tamanho m√≠nimo para descri√ß√£o
                        descricao = texto
                        break

        # 3. Buscar por meta tags
        if not descricao:
            meta_desc = soup.find("meta", attrs={"name": "description"})
            if meta_desc and meta_desc.get("content"):
                descricao = meta_desc["content"]

        logger.debug(
            f"Descri√ß√£o extra√≠da para curso {id_curso}: {len(descricao)} caracteres"
        )
        return descricao

    except Exception as e:
        logger.error(f"Erro ao extrair descri√ß√£o do curso {id_curso}: {e}")
        return ""


def extrair_palavras_chave_curso(id_curso: str, logger: logging.Logger) -> str:
    """Extrai palavras-chave do curso."""
    url_curso = f"https://www.unasus.gov.br/cursos/curso/{id_curso}"

    try:
        response = requests.get(url_curso, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.text, "html.parser")

        palavras_chave = ""

        # Buscar por meta tags de palavras-chave
        meta_keywords = soup.find("meta", attrs={"name": "keywords"})
        if meta_keywords and meta_keywords.get("content"):
            palavras_chave = meta_keywords["content"]

        # Buscar por tags ou elementos espec√≠ficos
        if not palavras_chave:
            elementos = soup.find_all(
                string=lambda t: t and "palavras-chave" in t.lower()
            )

            for elemento in elementos:
                parent = elemento.parent
                if parent:
                    texto = parent.get_text(strip=True)
                    if "palavras-chave" in texto.lower():
                        palavras_chave = texto
                        break

        logger.debug(
            f"Palavras-chave extra√≠das para curso {id_curso}: {palavras_chave}"
        )
        return palavras_chave

    except Exception as e:
        logger.error(f"Erro ao extrair palavras-chave do curso {id_curso}: {e}")
        return ""


def analisar_deia_completo(curso: Dict, logger: logging.Logger) -> Dict:
    """Realiza an√°lise DEIA completa em todos os campos do curso."""
    # Coleta todos os textos para an√°lise
    textos_para_analise = []

    # Campos b√°sicos
    titulo = curso.get("no_curso", "")
    descricao_curso = curso.get("ds_curso", "")
    descricao_oferta = curso.get("descricao_oferta", "")
    palavras_chave_curso = curso.get("palavras_chave_curso", "")
    palavras_chave_oferta = curso.get("palavras_chave", "")
    texto_pagina = curso.get("texto_pagina_inicial", "")

    # Adiciona todos os textos √† lista
    if titulo:
        textos_para_analise.append(f"T√≠tulo: {titulo}")
    if descricao_curso:
        textos_para_analise.append(f"Descri√ß√£o do curso: {descricao_curso}")
    if descricao_oferta:
        textos_para_analise.append(f"Descri√ß√£o da oferta: {descricao_oferta}")
    if palavras_chave_curso:
        textos_para_analise.append(f"Palavras-chave do curso: {palavras_chave_curso}")
    if palavras_chave_oferta:
        textos_para_analise.append(f"Palavras-chave da oferta: {palavras_chave_oferta}")
    if texto_pagina:
        textos_para_analise.append(f"Texto da p√°gina: {texto_pagina}")

    # Combina todos os textos
    texto_completo = " ".join(textos_para_analise)

    # Busca descritores DEIA
    descritor_encontrado = encontrar_descritor_deia_melhorado(texto_completo)

    # Atualiza o curso
    curso["tem_deia"] = "Sim" if descritor_encontrado else "N√£o"
    curso["deia_encontrado"] = descritor_encontrado
    curso["texto_analisado_deia"] = texto_completo[
        :1000
    ]  # Primeiros 1000 caracteres para debug

    if descritor_encontrado:
        logger.info(
            f"DEIA encontrado no curso {curso.get('co_seq_curso', 'N/A')}: {descritor_encontrado}"
        )

    return curso


def processar_curso_melhorado(
    curso: Dict, id_curso: str, logger: logging.Logger
) -> List[Dict]:
    """Processa um curso com coleta completa de dados."""
    # Extrai dados adicionais do curso
    logger.info(f"Processando curso {id_curso}: {curso.get('no_curso', 'N/A')}")

    # Extrai descri√ß√£o do curso se n√£o existir
    if not curso.get("ds_curso"):
        descricao = extrair_descricao_curso_melhorada(id_curso, logger)
        curso["ds_curso"] = descricao

    # Extrai palavras-chave do curso
    palavras_chave_curso = extrair_palavras_chave_curso(id_curso, logger)
    curso["palavras_chave_curso"] = palavras_chave_curso

    # Extrai texto da p√°gina inicial
    texto_pagina = extrair_texto_pagina_inicial(id_curso, logger)
    curso["texto_pagina_inicial"] = texto_pagina

    # Analisa DEIA em todos os campos
    curso = analisar_deia_completo(curso, logger)

    # Extrai ofertas (mant√©m a l√≥gica existente)
    ofertas = extrair_ofertas_do_curso(id_curso, logger)

    if not ofertas:
        logger.warning(f"Curso {id_curso} sem ofertas")
        return [{**curso, "id_oferta": "", "erro": "Sem ofertas encontradas"}]

    # Processa ofertas
    dados_ofertas = []
    for j, id_oferta in enumerate(ofertas, 1):
        logger.info(f"  Oferta {j}/{len(ofertas)}: {id_oferta}")
        dados_oferta = extrair_dados_oferta(id_oferta, logger)
        linha = {**curso, **dados_oferta}

        if "rank" in linha:
            del linha["rank"]

        dados_ofertas.append(linha)
        time.sleep(1)

    return dados_ofertas


# Fun√ß√µes auxiliares (mantidas do c√≥digo original)
def extrair_ofertas_do_curso(id_curso: str, logger: logging.Logger) -> List[str]:
    """Extrai ofertas de um curso espec√≠fico com logs detalhados."""
    url_curso = f"https://www.unasus.gov.br/cursos/curso/{id_curso}"

    try:
        logger.info(f"Buscando ofertas do curso {id_curso}...")
        resp = requests.get(url_curso, headers=HEADERS, timeout=30)

        if resp.status_code != 200:
            logger.error(f"Erro HTTP {resp.status_code} ao acessar curso {id_curso}")
            return []

        soup = BeautifulSoup(resp.text, "html.parser")
        ofertas = []

        # Buscar links de ofertas de v√°rias formas
        links_encontrados = []

        # 1. Buscar links diretos de ofertas
        for link in soup.find_all("a", href=True):
            href = link["href"]
            links_encontrados.append(href)

            # Verifica diferentes padr√µes de URL de oferta
            if any(
                pattern in href
                for pattern in ["/cursos/oferta/", "../oferta/", "oferta/"]
            ):
                # Extrai o ID da oferta do final da URL
                id_oferta = href.split("/")[-1]
                if id_oferta.isdigit():
                    ofertas.append(id_oferta)
                    logger.info(f"  ‚úÖ Oferta encontrada: {id_oferta}")

        # 2. Buscar por bot√µes ou links que possam mostrar ofertas encerradas
        botoes_encerradas = soup.find_all(
            "a", string=lambda t: t and "encerrada" in t.lower()
        )
        if botoes_encerradas:
            logger.info(
                f"  üìã Encontrados {len(botoes_encerradas)} links de ofertas encerradas"
            )
            for botao in botoes_encerradas:
                logger.info(f"    - {botao.get_text().strip()}")

            # Tentar acessar as ofertas encerradas
            ofertas_encerradas = buscar_ofertas_encerradas(soup, url_curso, logger)
            ofertas.extend(ofertas_encerradas)

        # 3. Buscar por divs que possam conter ofertas
        divs_oferta = soup.find_all("div", class_=lambda c: c and "oferta" in c.lower())
        if divs_oferta:
            logger.info(
                f"  üìã Encontrados {len(divs_oferta)} divs com 'oferta' na classe"
            )

        # 4. Verificar se h√° JavaScript que carrega ofertas dinamicamente
        scripts = soup.find_all("script")
        scripts_com_oferta = [
            s for s in scripts if s.string and "oferta" in s.string.lower()
        ]
        if scripts_com_oferta:
            logger.info(
                f"  üìã Encontrados {len(scripts_com_oferta)} scripts com 'oferta'"
            )

        ofertas_unicas = list(set(ofertas))  # Remove duplicados

        if ofertas_unicas:
            logger.info(
                f"  ‚úÖ Total de ofertas √∫nicas encontradas: {len(ofertas_unicas)}"
            )
        else:
            logger.warning(f"  ‚ùå Nenhuma oferta encontrada para o curso {id_curso}")
            logger.info(f"  üìã Total de links na p√°gina: {len(links_encontrados)}")
            logger.info(f"  üìã Primeiros 5 links: {links_encontrados[:5]}")

        return ofertas_unicas

    except Exception as e:
        logger.error(f"Erro ao buscar ofertas do curso {id_curso}: {e}")
        return []


def buscar_ofertas_encerradas(
    soup: BeautifulSoup, url_curso: str, logger: logging.Logger
) -> List[str]:
    """Busca ofertas encerradas na p√°gina do curso."""
    ofertas_encerradas = []

    try:
        # Procurar por links que contenham "encerrada" ou "encerradas"
        links_encerradas = soup.find_all(
            "a",
            href=True,
            string=lambda t: t
            and any(palavra in t.lower() for palavra in ["encerrada", "encerradas"]),
        )

        for link in links_encerradas:
            href = link.get("href")
            if href:
                # Se for um link relativo, construir URL completa
                if href.startswith("/"):
                    url_encerradas = f"https://www.unasus.gov.br{href}"
                elif href.startswith("http"):
                    url_encerradas = href
                else:
                    # URL relativa ao curso
                    url_encerradas = f"{url_curso}/{href}"

                logger.info(f"  üîç Acessando ofertas encerradas: {url_encerradas}")

                # Acessar a p√°gina de ofertas encerradas
                resp_encerradas = requests.get(
                    url_encerradas, headers=HEADERS, timeout=30
                )
                if resp_encerradas.status_code == 200:
                    soup_encerradas = BeautifulSoup(resp_encerradas.text, "html.parser")

                    # Buscar links de ofertas na p√°gina de encerradas
                    for link_oferta in soup_encerradas.find_all("a", href=True):
                        href_oferta = link_oferta["href"]
                        if "/cursos/oferta/" in href_oferta:
                            id_oferta = href_oferta.split("/")[-1]
                            if id_oferta.isdigit():
                                ofertas_encerradas.append(id_oferta)
                                logger.info(
                                    f"    ‚úÖ Oferta encerrada encontrada: {id_oferta}"
                                )

        return ofertas_encerradas

    except Exception as e:
        logger.error(f"Erro ao buscar ofertas encerradas: {e}")
        return []


def extrair_dados_oferta(id_oferta: str, logger: logging.Logger) -> Dict:
    """Extrai dados de uma oferta usando a API REST com fallback para HTML."""
    url_oferta = f"https://www.unasus.gov.br/cursos/oferta/{id_oferta}"
    url_api = f"https://www.unasus.gov.br/cursos/rest/oferta/{id_oferta}"

    try:
        logger.info(f"  üîç Extraindo dados da oferta {id_oferta}...")

        # Primeiro, tentar a API REST
        api_headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/138.0.0.0 Safari/537.36"
            ),
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "X-Requested-With": "XMLHttpRequest",
            "Referer": url_oferta,
        }

        dados = {"id_oferta": id_oferta, "url_oferta": url_oferta}
        dados["codigo_oferta"] = id_oferta

        # Tentar API REST primeiro
        try:
            resp_api = requests.get(url_api, headers=api_headers, timeout=30)
            if resp_api.status_code == 200:
                response_data = resp_api.json()
                logger.info("    ‚úÖ Dados obtidos via API REST")

                # Os dados est√£o dentro do campo 'data'
                oferta_data = response_data.get("data", {})

                # Extrair dados da resposta JSON
                dados["vagas"] = str(oferta_data.get("qt_vaga", ""))
                dados["publico_alvo"] = oferta_data.get("ds_publico_alvo", "")
                dados["local_oferta"] = oferta_data.get("no_local_oferta", "")
                dados["formato"] = oferta_data.get("no_formato", "")

                # Programas de governo podem ser uma lista
                programas = oferta_data.get("no_programas_governo", [])
                if isinstance(programas, list):
                    dados["programas_governo"] = ", ".join(programas)
                else:
                    dados["programas_governo"] = str(programas) if programas else ""

                # Temas podem ser uma lista
                temas = oferta_data.get("no_temas", [])
                if isinstance(temas, list):
                    dados["temas"] = ", ".join(temas)
                else:
                    dados["temas"] = str(temas) if temas else ""

                # DeCs podem ser uma lista
                decs = oferta_data.get("no_decs", [])
                if isinstance(decs, list):
                    dados["decs"] = ", ".join(decs)
                else:
                    dados["decs"] = str(decs) if decs else ""

                dados["descricao_oferta"] = oferta_data.get("ds_oferta", "")

                # Palavras-chave podem ser uma lista
                palavras_chave = oferta_data.get("no_palavras_chave", [])
                if isinstance(palavras_chave, list):
                    dados["palavras_chave"] = ", ".join(palavras_chave)
                else:
                    dados["palavras_chave"] = (
                        str(palavras_chave) if palavras_chave else ""
                    )

                if dados["vagas"]:
                    logger.info(f"    ‚úÖ Vagas extra√≠das: {dados['vagas']}")
                else:
                    logger.warning("    ‚ö†Ô∏è Vagas n√£o encontradas na API")

                return dados
            else:
                logger.warning(f"    ‚ö†Ô∏è API REST retornou status {resp_api.status_code}")
        except Exception as e:
            logger.warning(f"    ‚ö†Ô∏è Erro na API REST: {e}")

        # Fallback: tentar extrair da p√°gina HTML
        logger.info("    üîÑ Tentando extra√ß√£o da p√°gina HTML...")
        resp = requests.get(url_oferta, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(resp.text, "html.parser")

        # Buscar o div principal com os dados da oferta
        oferta_quadro = soup.find("div", id="oferta_quadro")
        if oferta_quadro:
            texto_completo = oferta_quadro.get_text()

            # Extrair vagas usando regex para encontrar o padr√£o "Vagas: n√∫mero"
            vagas_match = re.search(r"Vagas:\s*(\d+)", texto_completo)
            dados["vagas"] = vagas_match.group(1) if vagas_match else ""

            # Extrair p√∫blico-alvo
            publico_match = re.search(
                r"P√∫blico-alvo:\s*(.*?)(?=\n\n|\nLocal|\nFormato|\nN√≠vel|\nModalidade|\nProgramas|\nTemas|\nDeCs|\nDescri√ß√£o|\nPalavras-chave|$)",
                texto_completo,
                re.DOTALL,
            )
            dados["publico_alvo"] = (
                publico_match.group(1).strip() if publico_match else ""
            )

            # Extrair local da oferta
            local_match = re.search(
                r"Local da Oferta:\s*(.*?)(?=\n\n|\nFormato|\nN√≠vel|\nModalidade|\nProgramas|\nTemas|\nDeCs|\nDescri√ß√£o|\nPalavras-chave|$)",
                texto_completo,
                re.DOTALL,
            )
            dados["local_oferta"] = local_match.group(1).strip() if local_match else ""

            # Extrair formato
            formato_match = re.search(
                r"Formato:\s*(.*?)(?=\n\n|\nN√≠vel|\nModalidade|\nProgramas|\nTemas|\nDeCs|\nDescri√ß√£o|\nPalavras-chave|$)",
                texto_completo,
                re.DOTALL,
            )
            dados["formato"] = formato_match.group(1).strip() if formato_match else ""

            # Extrair programas de governo
            programas_match = re.search(
                r"Programas de governo:\s*(.*?)(?=\n\n|\nTemas|\nDeCs|\nDescri√ß√£o|\nPalavras-chave|$)",
                texto_completo,
                re.DOTALL,
            )
            dados["programas_governo"] = (
                programas_match.group(1).strip() if programas_match else ""
            )

            # Extrair temas
            temas_match = re.search(
                r"Temas:\s*(.*?)(?=\n\n|\nDeCs|\nDescri√ß√£o|\nPalavras-chave|$)",
                texto_completo,
                re.DOTALL,
            )
            dados["temas"] = temas_match.group(1).strip() if temas_match else ""

            # Extrair DeCs
            decs_match = re.search(
                r"DeCs:\s*(.*?)(?=\n\n|\nDescri√ß√£o|\nPalavras-chave|$)",
                texto_completo,
                re.DOTALL,
            )
            dados["decs"] = decs_match.group(1).strip() if decs_match else ""

            # Extrair descri√ß√£o da oferta
            descricao_match = re.search(
                r"Descri√ß√£o da oferta:\s*(.*?)(?=\n\n|\nPalavras-chave|$)",
                texto_completo,
                re.DOTALL,
            )
            dados["descricao_oferta"] = (
                descricao_match.group(1).strip() if descricao_match else ""
            )

            # Extrair palavras-chave
            palavras_match = re.search(
                r"Palavras-chave:\s*(.*?)(?=\n\n|$)", texto_completo, re.DOTALL
            )
            dados["palavras_chave"] = (
                palavras_match.group(1).strip() if palavras_match else ""
            )

        else:
            # Fallback para estrutura antiga (tabela)
            dados["vagas"] = ""
            dados["publico_alvo"] = ""
            dados["local_oferta"] = ""
            dados["formato"] = ""
            dados["programas_governo"] = ""
            dados["temas"] = ""
            dados["decs"] = ""
            dados["descricao_oferta"] = ""
            dados["palavras_chave"] = ""

            # Tentar extrair usando a estrutura de tabela antiga
            vagas = soup.find(string=lambda t: t and "Vagas" in t)
            if vagas:
                try:
                    dados["vagas"] = vagas.parent.find_next("td").text.strip()
                except Exception:
                    dados["vagas"] = ""

            publico = soup.find(string=lambda t: t and "P√∫blico-alvo" in t)
            if publico:
                try:
                    dados["publico_alvo"] = publico.parent.find_next("td").text.strip()
                except Exception:
                    dados["publico_alvo"] = ""

            local = soup.find(string=lambda t: t and "Local" in t)
            if local:
                try:
                    dados["local_oferta"] = local.parent.find_next("td").text.strip()
                except Exception:
                    dados["local_oferta"] = ""

            formato = soup.find(string=lambda t: t and "Formato" in t)
            if formato:
                try:
                    dados["formato"] = formato.parent.find_next("td").text.strip()
                except Exception:
                    dados["formato"] = ""

            programas = soup.find(string=lambda t: t and "Programas de governo" in t)
            if programas:
                try:
                    dados["programas_governo"] = programas.parent.find_next(
                        "td"
                    ).text.strip()
                except Exception:
                    dados["programas_governo"] = ""

            temas = soup.find(string=lambda t: t and "Temas" in t)
            if temas:
                try:
                    dados["temas"] = temas.parent.find_next("td").text.strip()
                except Exception:
                    dados["temas"] = ""

            decs = soup.find(string=lambda t: t and "DeCs" in t)
            if decs:
                try:
                    dados["decs"] = decs.parent.find_next("td").text.strip()
                except Exception:
                    dados["decs"] = ""

            descricao = soup.find(string=lambda t: t and "Descri√ß√£o da oferta" in t)
            if descricao:
                try:
                    dados["descricao_oferta"] = descricao.parent.find_next(
                        "td"
                    ).text.strip()
                except Exception:
                    dados["descricao_oferta"] = ""

            palavras = soup.find(string=lambda t: t and "Palavras-chave" in t)
            if palavras:
                try:
                    dados["palavras_chave"] = palavras.parent.find_next(
                        "td"
                    ).text.strip()
                except Exception:
                    dados["palavras_chave"] = ""

        logger.debug(f"Oferta {id_oferta}: dados extra√≠dos com sucesso")
        return dados

    except Exception as e:
        logger.error(f"Erro ao buscar dados da oferta {id_oferta}: {e}")
        return {"id_oferta": id_oferta, "erro": str(e)}


def main():
    """Fun√ß√£o principal do scraper melhorado."""
    logger = setup_logging()
    logger.info("=== INICIANDO SCRAPER UNA-SUS MELHORADO ===")

    # Configura√ß√µes
    csv_path = "unasus_ofertas_melhoradas.csv"
    lote = 10
    todos_detalhes = []
    pagina = 0
    cursos_processados = set()

    # Carregar dados existentes
    if os.path.exists(csv_path):
        try:
            df_existente = pd.read_csv(csv_path, encoding="utf-8-sig")
            if "co_seq_curso" in df_existente.columns:
                cursos_processados = set(
                    df_existente["co_seq_curso"].astype(str).unique()
                )
            logger.info(f"Cursos j√° processados: {len(cursos_processados)}")
        except Exception as e:
            logger.error(f"Erro ao carregar CSV existente: {e}")

    payload = PAYLOAD_INICIAL.copy()
    logger.info(f"Arquivo de sa√≠da: {csv_path}")

    # Loop principal
    while True:
        try:
            logger.info(f"=== PROCESSANDO P√ÅGINA {pagina + 1} ===")

            response = requests.post(
                URL,
                data=payload,
                headers=HEADERS,
                cookies=COOKIES,
                timeout=30,
            )

            data = response.json()
            itens = data.get("results", {}).get("itens", [])

            if not itens:
                logger.info("Nenhum item encontrado. Finalizando.")
                break

            for curso in itens:
                id_curso = (
                    curso.get("co_seq_curso")
                    or curso.get("id_curso")
                    or curso.get("co_curso")
                    or curso.get("id")
                )

                if not id_curso:
                    continue

                id_curso_str = str(id_curso)
                if id_curso_str in cursos_processados:
                    continue

                # Processa o curso com as melhorias
                dados_curso = processar_curso_melhorado(curso, id_curso_str, logger)
                todos_detalhes.extend(dados_curso)
                cursos_processados.add(id_curso_str)

                # Salvamento incremental
                if len(todos_detalhes) >= lote:
                    if os.path.exists(csv_path):
                        df_existente = pd.read_csv(csv_path, encoding="utf-8-sig")
                        df_novo = pd.DataFrame(todos_detalhes)
                        df_final = pd.concat([df_existente, df_novo], ignore_index=True)
                    else:
                        df_final = pd.DataFrame(todos_detalhes)

                    df_final.to_csv(csv_path, index=False, encoding="utf-8-sig")
                    logger.info(
                        f"Progresso salvo: {len(cursos_processados)} cursos processados"
                    )
                    todos_detalhes = []

            pagina += 1
            proximo = data.get("results", {}).get("proximo")
            if not proximo:
                break
            payload["proximo"] = proximo
            time.sleep(1)

        except Exception as e:
            logger.error(f"Erro de conex√£o: {e}. Tentando novamente em 30 segundos...")
            time.sleep(30)
            continue

    # Salva o restante
    if todos_detalhes:
        if os.path.exists(csv_path):
            df_existente = pd.read_csv(csv_path, encoding="utf-8-sig")
            df_novo = pd.DataFrame(todos_detalhes)
            df_final = pd.concat([df_existente, df_novo], ignore_index=True)
        else:
            df_final = pd.DataFrame(todos_detalhes)

        df_final.to_csv(csv_path, index=False, encoding="utf-8-sig")
        logger.info(f"Finalizado! Todos os dados salvos em {csv_path}")

    # Gera relat√≥rio final
    try:
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
        logger.info("=== RELAT√ìRIO FINAL ===")
        logger.info(f"Total de registros: {len(df)}")
        logger.info(f"Colunas: {list(df.columns)}")

        if "tem_deia" in df.columns:
            deia_stats = df["tem_deia"].value_counts()
            logger.info(f"Cursos com DEIA: {deia_stats.get('Sim', 0)}")
            logger.info(f"Cursos sem DEIA: {deia_stats.get('N√£o', 0)}")

            # Mostra alguns exemplos de cursos com DEIA
            if deia_stats.get("Sim", 0) > 0:
                cursos_deia = df[df["tem_deia"] == "Sim"]
                logger.info("Exemplos de cursos com DEIA:")
                for _, curso in cursos_deia.head(5).iterrows():
                    logger.info(
                        f"  - {curso.get('no_curso', 'N/A')}: {curso.get('deia_encontrado', 'N/A')}"
                    )

        logger.info("=== SCRAPER FINALIZADO ===")

    except Exception as e:
        logger.error(f"Erro ao gerar relat√≥rio final: {e}")


if __name__ == "__main__":
    main()
