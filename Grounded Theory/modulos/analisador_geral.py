#!/usr/bin/env python3
"""
üìä Analisador Geral - An√°lises Flex√≠veis de Dados
=================================================

Este m√≥dulo implementa an√°lises gerais e flex√≠veis dos dados coletados,
permitindo diferentes tipos de an√°lise sem comprometer a integridade
dos dados originais.

üéØ PRINC√çPIOS:
- Trabalha com dados j√° coletados
- N√ÉO modifica dados originais
- An√°lises flex√≠veis e configur√°veis
- Preserva database fiel
- M√∫ltiplos tipos de an√°lise

üî¨ TIPOS DE AN√ÅLISE:
- An√°lise estat√≠stica geral
- An√°lise por categorias
- An√°lise temporal
- An√°lise geogr√°fica
- An√°lise de conte√∫do
- An√°lise comparativa
- An√°lise customizada
"""

import json
import logging
import os
import re
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd


class AnalisadorGeral:
    """
    üìä Analisador Geral para Dados UNA-SUS

    Realiza an√°lises flex√≠veis e configur√°veis dos dados coletados.
    """

    def __init__(self, logger: logging.Logger = None):
        """
        Inicializa o analisador geral.

        Args:
            logger: Logger para acompanhamento
        """
        self.logger = logger or self._configurar_logger()
        self.dados_originais = []
        self.dados_processados = []
        self.resultados_analise = {}
        self.configuracoes_analise = {}

    def _configurar_logger(self) -> logging.Logger:
        """
        üìù Configura o logger para o analisador.

        Returns:
            Logger configurado
        """
        # Criar pasta de logs se n√£o existir
        os.makedirs("logs", exist_ok=True)

        # Configurar logger
        logger = logging.getLogger("AnalisadorGeral")
        logger.setLevel(logging.INFO)

        # Handler para arquivo
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        fh = logging.FileHandler(
            f"logs/analisador_geral_{timestamp}.log", encoding="utf-8"
        )
        fh.setLevel(logging.INFO)

        # Handler para console
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)

        # Formato
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)

        # Adicionar handlers
        logger.addHandler(fh)
        logger.addHandler(ch)

        return logger

    def carregar_dados_para_analise(self, caminho_arquivo: str) -> bool:
        """
        üìÇ Carrega dados para an√°lise.

        Args:
            caminho_arquivo: Caminho para o arquivo de dados

        Returns:
            True se carregado com sucesso
        """
        try:
            self.logger.info(f"üîÑ Carregando dados de: {caminho_arquivo}")

            # Tentar carregar CSV primeiro
            if caminho_arquivo.endswith(".csv"):
                df = pd.read_csv(caminho_arquivo, encoding="utf-8")
                self.dados_originais = df.to_dict("records")

            # Tentar carregar JSON
            elif caminho_arquivo.endswith(".json"):
                with open(caminho_arquivo, "r", encoding="utf-8") as f:
                    self.dados_originais = json.load(f)

            # Tentar carregar Excel (se dispon√≠vel)
            elif caminho_arquivo.endswith((".xlsx", ".xls")):
                try:
                    df = pd.read_excel(caminho_arquivo)
                    self.dados_originais = df.to_dict("records")
                except ImportError:
                    raise ValueError(
                        "‚ùå openpyxl n√£o est√° instalado. "
                        "Instale com: pip install openpyxl"
                    )

            else:
                raise ValueError(
                    f"‚ùå Formato de arquivo n√£o suportado: {caminho_arquivo}"
                )

            self.logger.info(
                f"‚úÖ Dados carregados: {len(self.dados_originais)} registros"
            )
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar dados: {str(e)}")
            return False

    def configurar_analise(self, tipo_analise: str, parametros: Dict = None):
        """
        ‚öôÔ∏è Configura o tipo de an√°lise a ser realizada.

        Args:
            tipo_analise: Tipo de an√°lise ('estatistica', 'categoria', 'temporal', etc.)
            parametros: Par√¢metros espec√≠ficos da an√°lise
        """
        self.configuracoes_analise = {
            "tipo": tipo_analise,
            "parametros": parametros or {},
            "timestamp": datetime.now().isoformat(),
        }

        self.logger.info(f"‚öôÔ∏è An√°lise configurada: {tipo_analise}")

    def executar_analise(self) -> Dict:
        """
        üöÄ Executa a an√°lise configurada.

        Returns:
            Resultados da an√°lise
        """
        if not self.dados_originais:
            self.logger.error("‚ùå Nenhum dado carregado para an√°lise")
            return {}

        tipo_analise = self.configuracoes_analise.get("tipo", "estatistica")

        self.logger.info(f"üöÄ Executando an√°lise: {tipo_analise}")

        if tipo_analise == "estatistica":
            return self._analise_estatistica_geral()
        elif tipo_analise == "categoria":
            return self._analise_por_categoria()
        elif tipo_analise == "temporal":
            return self._analise_temporal()
        elif tipo_analise == "geografica":
            return self._analise_geografica()
        elif tipo_analise == "conteudo":
            return self._analise_conteudo()
        elif tipo_analise == "comparativa":
            return self._analise_comparativa()
        elif tipo_analise == "customizada":
            return self._analise_customizada()
        else:
            self.logger.warning(f"‚ö†Ô∏è Tipo de an√°lise n√£o reconhecido: {tipo_analise}")
            return self._analise_estatistica_geral()

    def _analise_estatistica_geral(self) -> Dict:
        """
        üìä An√°lise estat√≠stica geral dos dados.

        Returns:
            Estat√≠sticas gerais
        """
        self.logger.info("üìä Executando an√°lise estat√≠stica geral")

        if not self.dados_originais:
            return {}

        df = pd.DataFrame(self.dados_originais)

        estatisticas = {
            "resumo_geral": {
                "total_cursos": len(df),
                "campos_disponiveis": list(df.columns),
                "campos_preenchidos": df.count().to_dict(),
                "percentual_preenchimento": (df.count() / len(df) * 100).to_dict(),
            },
            "estatisticas_numericas": {},
            "estatisticas_categoricas": {},
            "valores_unicos": {},
        }

        # An√°lise de campos num√©ricos
        campos_numericos = df.select_dtypes(include=["number"]).columns
        for campo in campos_numericos:
            estatisticas["estatisticas_numericas"][campo] = {
                "media": df[campo].mean(),
                "mediana": df[campo].median(),
                "min": df[campo].min(),
                "max": df[campo].max(),
                "desvio_padrao": df[campo].std(),
            }

            # An√°lise de campos categ√≥ricos
        campos_categoricos = df.select_dtypes(include=["object"]).columns
        for campo in campos_categoricos:
            # Pular campos que cont√™m dicion√°rios ou objetos complexos
            if campo == "metadata_coleta":
                estatisticas["estatisticas_categoricas"][campo] = {
                    "valores_unicos": "N/A (campo com dicion√°rios)",
                    "top_valores": {},
                    "valores_nulos": df[campo].isnull().sum(),
                    "tipo": "metadata_complexa",
                }
                continue

            try:
                valores_unicos = df[campo].nunique()
                top_valores = df[campo].value_counts().head(10).to_dict()
            except (TypeError, ValueError):
                # Se o campo cont√©m objetos n√£o-hashable (como dicion√°rios)
                valores_unicos = len(df[campo].dropna().unique())
                top_valores = {}

            estatisticas["estatisticas_categoricas"][campo] = {
                "valores_unicos": valores_unicos,
                "top_valores": top_valores,
                "valores_nulos": df[campo].isnull().sum(),
            }

        # Valores √∫nicos por campo
        for campo in df.columns:
            if campo == "metadata_coleta":
                estatisticas["valores_unicos"][campo] = "N/A (campo com dicion√°rios)"
                continue

            try:
                estatisticas["valores_unicos"][campo] = df[campo].nunique()
            except (TypeError, ValueError):
                # Se o campo cont√©m objetos n√£o-hashable
                estatisticas["valores_unicos"][campo] = len(df[campo].dropna().unique())

        self.resultados_analise = estatisticas
        return estatisticas

    def _analise_por_categoria(self) -> Dict:
        """
        üìÇ An√°lise por categorias espec√≠ficas.

        Returns:
            An√°lise por categorias
        """
        self.logger.info("üìÇ Executando an√°lise por categoria")

        parametros = self.configuracoes_analise.get("parametros", {})
        campo_categoria = parametros.get("campo_categoria", "area_tematica")

        df = pd.DataFrame(self.dados_originais)

        if campo_categoria not in df.columns:
            self.logger.warning(
                f"‚ö†Ô∏è Campo de categoria n√£o encontrado: {campo_categoria}"
            )
            return {}

        try:
            categorias_encontradas = df[campo_categoria].value_counts().to_dict()
        except (TypeError, ValueError):
            # Se o campo cont√©m objetos n√£o-hashable
            categorias_encontradas = {}

        analise_categoria = {
            "campo_analisado": campo_categoria,
            "categorias_encontradas": categorias_encontradas,
            "estatisticas_por_categoria": {},
        }

        # Estat√≠sticas por categoria
        try:
            categorias_unicas = df[campo_categoria].unique()
        except (TypeError, ValueError):
            # Se o campo cont√©m objetos n√£o-hashable
            categorias_unicas = df[campo_categoria].dropna().unique()

        for categoria in categorias_unicas:
            if pd.isna(categoria):
                continue

            dados_categoria = df[df[campo_categoria] == categoria]
            analise_categoria["estatisticas_por_categoria"][str(categoria)] = {
                "quantidade": len(dados_categoria),
                "percentual": len(dados_categoria) / len(df) * 100,
            }

        self.resultados_analise = analise_categoria
        return analise_categoria

    def _analise_temporal(self) -> Dict:
        """
        üìÖ An√°lise temporal dos dados.

        Returns:
            An√°lise temporal
        """
        self.logger.info("üìÖ Executando an√°lise temporal")

        df = pd.DataFrame(self.dados_originais)

        # Identificar campos de data
        campos_data = []
        for coluna in df.columns:
            if any(
                palavra in coluna.lower()
                for palavra in ["data", "inicio", "fim", "ano", "mes"]
            ):
                campos_data.append(coluna)

        analise_temporal = {
            "campos_data_encontrados": campos_data,
            "analise_por_campo": {},
        }

        for campo in campos_data:
            try:
                # Tentar converter para datetime
                df[campo] = pd.to_datetime(df[campo], errors="coerce")

                analise_temporal["analise_por_campo"][campo] = {
                    "primeira_data": df[campo].min(),
                    "ultima_data": df[campo].max(),
                    "periodo_dias": (df[campo].max() - df[campo].min()).days,
                    "distribuicao_anual": df[campo].dt.year.value_counts().to_dict(),
                }
            except:
                analise_temporal["analise_por_campo"][campo] = {
                    "erro": "N√£o foi poss√≠vel converter para data"
                }

        self.resultados_analise = analise_temporal
        return analise_temporal

    def _analise_geografica(self) -> Dict:
        """
        üåç An√°lise geogr√°fica dos dados.

        Returns:
            An√°lise geogr√°fica
        """
        self.logger.info("üåç Executando an√°lise geogr√°fica")

        df = pd.DataFrame(self.dados_originais)

        # Identificar campos geogr√°ficos
        campos_geo = []
        for coluna in df.columns:
            if any(
                palavra in coluna.lower()
                for palavra in ["estado", "cidade", "regiao", "uf", "local"]
            ):
                campos_geo.append(coluna)

        analise_geografica = {
            "campos_geograficos_encontrados": campos_geo,
            "distribuicao_geografica": {},
        }

        for campo in campos_geo:
            analise_geografica["distribuicao_geografica"][campo] = {
                "valores_unicos": df[campo].nunique(),
                "top_localizacoes": df[campo].value_counts().head(10).to_dict(),
                "valores_nulos": df[campo].isnull().sum(),
            }

        self.resultados_analise = analise_geografica
        return analise_geografica

    def _analise_conteudo(self) -> Dict:
        """
        üìù An√°lise de conte√∫do dos dados.

        Returns:
            An√°lise de conte√∫do
        """
        self.logger.info("üìù Executando an√°lise de conte√∫do")

        df = pd.DataFrame(self.dados_originais)

        # Identificar campos de texto
        campos_texto = []
        for coluna in df.columns:
            if any(
                palavra in coluna.lower()
                for palavra in ["descricao", "titulo", "nome", "texto", "conteudo"]
            ):
                campos_texto.append(coluna)

        analise_conteudo = {
            "campos_texto_encontrados": campos_texto,
            "analise_por_campo": {},
        }

        for campo in campos_texto:
            textos = df[campo].dropna().astype(str)

            analise_conteudo["analise_por_campo"][campo] = {
                "total_textos": len(textos),
                "media_caracteres": textos.str.len().mean(),
                "min_caracteres": textos.str.len().min(),
                "max_caracteres": textos.str.len().max(),
                "palavras_mais_comuns": self._extrair_palavras_comuns(textos),
            }

        self.resultados_analise = analise_conteudo
        return analise_conteudo

    def _analise_comparativa(self) -> Dict:
        """
        ‚öñÔ∏è An√°lise comparativa dos dados.

        Returns:
            An√°lise comparativa
        """
        self.logger.info("‚öñÔ∏è Executando an√°lise comparativa")

        parametros = self.configuracoes_analise.get("parametros", {})
        campos_comparacao = parametros.get("campos_comparacao", [])

        df = pd.DataFrame(self.dados_originais)

        analise_comparativa = {
            "campos_comparacao": campos_comparacao,
            "comparacoes": {},
        }

        for campo in campos_comparacao:
            if campo in df.columns:
                analise_comparativa["comparacoes"][campo] = {
                    "valores_unicos": df[campo].nunique(),
                    "distribuicao": df[campo].value_counts().to_dict(),
                    "correlacoes": self._calcular_correlacoes(df, campo),
                }

        self.resultados_analise = analise_comparativa
        return analise_comparativa

    def _analise_customizada(self) -> Dict:
        """
        üîß An√°lise customizada baseada em par√¢metros espec√≠ficos.

        Returns:
            An√°lise customizada
        """
        self.logger.info("üîß Executando an√°lise customizada")

        parametros = self.configuracoes_analise.get("parametros", {})
        filtros = parametros.get("filtros", {})
        campos_analise = parametros.get("campos_analise", [])

        df = pd.DataFrame(self.dados_originais)

        # Aplicar filtros
        for campo, valor in filtros.items():
            if campo in df.columns:
                df = df[df[campo] == valor]

        analise_customizada = {
            "filtros_aplicados": filtros,
            "registros_filtrados": len(df),
            "analise_campos": {},
        }

        for campo in campos_analise:
            if campo in df.columns:
                analise_customizada["analise_campos"][campo] = {
                    "valores_unicos": df[campo].nunique(),
                    "distribuicao": df[campo].value_counts().to_dict(),
                }

        self.resultados_analise = analise_customizada
        return analise_customizada

    def _extrair_palavras_comuns(self, textos: pd.Series, top_n: int = 10) -> Dict:
        """
        üìö Extrai palavras mais comuns dos textos.

        Args:
            textos: S√©rie com textos
            top_n: N√∫mero de palavras mais comuns

        Returns:
            Dicion√°rio com palavras e frequ√™ncias
        """
        todas_palavras = []
        for texto in textos:
            palavras = re.findall(r"\b\w+\b", texto.lower())
            todas_palavras.extend(palavras)

        from collections import Counter

        contador = Counter(todas_palavras)
        return dict(contador.most_common(top_n))

    def _calcular_correlacoes(self, df: pd.DataFrame, campo: str) -> Dict:
        """
        üìà Calcula correla√ß√µes para um campo espec√≠fico.

        Args:
            df: DataFrame com dados
            campo: Campo para an√°lise

        Returns:
            Correla√ß√µes encontradas
        """
        correlacoes = {}

        # Tentar calcular correla√ß√µes com campos num√©ricos
        campos_numericos = df.select_dtypes(include=["number"]).columns
        for campo_num in campos_numericos:
            if campo_num != campo:
                try:
                    correlacao = (
                        df[campo].astype("category").cat.codes.corr(df[campo_num])
                    )
                    if not pd.isna(correlacao):
                        correlacoes[campo_num] = correlacao
                except:
                    continue

        return correlacoes

    def salvar_resultados(self, caminho_saida: str = None):
        """
        üíæ Salva os resultados da an√°lise.

        Args:
            caminho_saida: Caminho para salvar os resultados
        """
        if not self.resultados_analise:
            self.logger.warning("‚ö†Ô∏è Nenhum resultado para salvar")
            return

        # Criar pasta de relat√≥rios se n√£o existir
        os.makedirs("relatorios", exist_ok=True)

        # Gerar nome do arquivo
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        tipo_analise = self.configuracoes_analise.get("tipo", "geral")

        if not caminho_saida:
            caminho_saida = f"relatorios/analise_{tipo_analise}_{timestamp}"

        # Salvar JSON
        caminho_json = f"{caminho_saida}.json"
        with open(caminho_json, "w", encoding="utf-8") as f:
            json.dump(
                self.resultados_analise, f, indent=2, ensure_ascii=False, default=str
            )

        # Salvar CSV (se aplic√°vel)
        if (
            isinstance(self.resultados_analise, dict)
            and "resumo_geral" in self.resultados_analise
        ):
            caminho_csv = f"{caminho_saida}.csv"
            df_resultado = pd.DataFrame([self.resultados_analise["resumo_geral"]])
            df_resultado.to_csv(caminho_csv, index=False, encoding="utf-8")

        # Gerar relat√≥rio Markdown
        caminho_md = f"{caminho_saida}.md"
        self._gerar_relatorio_markdown(caminho_md)

        self.logger.info(f"‚úÖ Resultados salvos em: {caminho_saida}.*")

    def _gerar_relatorio_markdown(self, caminho_arquivo: str):
        """
        üìÑ Gera relat√≥rio em Markdown.

        Args:
            caminho_arquivo: Caminho para o arquivo Markdown
        """
        timestamp = datetime.now().strftime("%d/%m/%Y %H:%M:%S")
        tipo_analise = self.configuracoes_analise.get("tipo", "geral")

        relatorio = f"""# üìä Relat√≥rio de An√°lise Geral - UNA-SUS

## üìÖ Informa√ß√µes Gerais
- **Data da An√°lise**: {timestamp}
- **Tipo de An√°lise**: {tipo_analise}
- **Total de Registros**: {len(self.dados_originais)}

## üéØ Resultados da An√°lise

### üìà Resumo Geral
"""

        if "resumo_geral" in self.resultados_analise:
            resumo = self.resultados_analise["resumo_geral"]
            relatorio += f"""
- **Total de Cursos**: {resumo.get('total_cursos', 'N/A')}
- **Campos Dispon√≠veis**: {len(resumo.get('campos_disponiveis', []))}
- **Campos com Dados**: {sum(1 for v in resumo.get('campos_preenchidos', {}).values() if v > 0)}
"""

        if "estatisticas_numericas" in self.resultados_analise:
            relatorio += "\n### üìä Estat√≠sticas Num√©ricas\n"
            for campo, stats in self.resultados_analise[
                "estatisticas_numericas"
            ].items():
                relatorio += f"""
#### {campo}
- **M√©dia**: {stats.get('media', 'N/A'):.2f}
- **Mediana**: {stats.get('mediana', 'N/A'):.2f}
- **M√≠nimo**: {stats.get('min', 'N/A')}
- **M√°ximo**: {stats.get('max', 'N/A')}
- **Desvio Padr√£o**: {stats.get('desvio_padrao', 'N/A'):.2f}
"""

        if "estatisticas_categoricas" in self.resultados_analise:
            relatorio += "\n### üìÇ Estat√≠sticas Categ√≥ricas\n"
            for campo, stats in self.resultados_analise[
                "estatisticas_categoricas"
            ].items():
                relatorio += f"""
#### {campo}
- **Valores √önicos**: {stats.get('valores_unicos', 'N/A')}
- **Valores Nulos**: {stats.get('valores_nulos', 'N/A')}
- **Top 5 Valores**: {list(stats.get('top_valores', {}).keys())[:5]}
"""

        relatorio += f"""

## üîß Configura√ß√µes da An√°lise
```json
{json.dumps(self.configuracoes_analise, indent=2, ensure_ascii=False)}
```

---
*Relat√≥rio gerado automaticamente pelo Analisador Geral UNA-SUS*
"""

        with open(caminho_arquivo, "w", encoding="utf-8") as f:
            f.write(relatorio)

        self.logger.info(f"üìÑ Relat√≥rio Markdown gerado: {caminho_arquivo}")


def main():
    """
    üöÄ Fun√ß√£o principal para teste do analisador.
    """
    print("üîç Analisador Geral UNA-SUS")
    print("=" * 50)

    # Exemplo de uso
    analisador = AnalisadorGeral()

    # Carregar dados (exemplo)
    # analisador.carregar_dados_para_analise("dados/unasus_dados_completos.json")

    # Configurar an√°lise
    analisador.configurar_analise("estatistica")

    # Executar an√°lise
    # resultados = analisador.executar_analise()

    # Salvar resultados
    # analisador.salvar_resultados()

    print("‚úÖ Analisador configurado e pronto para uso!")


if __name__ == "__main__":
    main()
