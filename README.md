# ğŸ¥ Scraper UNA-SUS - Coleta Inteligente de Dados Educacionais

> **ğŸ“š Projeto Educacional para Pesquisa em SaÃºde PÃºblica**  
> Um sistema inteligente e didÃ¡tico para coletar e analisar dados de cursos da Universidade Aberta do SUS (UNA-SUS), com foco especial em **Diversidade, Equidade, InclusÃ£o e Acessibilidade (DEIA)**.

## ğŸ¯ Sobre Este Projeto

### ğŸ’¡ **O que Ã©?**
Este Ã© um **scraper** (coletor automÃ¡tico de dados) que extrai informaÃ§Ãµes detalhadas de cursos da plataforma UNA-SUS. Ele foi desenvolvido para facilitar pesquisas acadÃªmicas e anÃ¡lises sobre educaÃ§Ã£o em saÃºde pÃºblica.

### ğŸ“ **Para quem Ã©?**
- **Pesquisadores** em saÃºde pÃºblica e educaÃ§Ã£o
- **Estudantes** de graduaÃ§Ã£o e pÃ³s-graduaÃ§Ã£o
- **Desenvolvedores** interessados em web scraping
- **Analistas de dados** em saÃºde
- **Qualquer pessoa** interessada em dados educacionais

### ğŸš€ **Por que usar?**
- âœ… **Automatiza** a coleta de dados que seria manual
- âœ… **Identifica** cursos com foco em DEIA automaticamente
- âœ… **Organiza** dados em formato estruturado (CSV)
- âœ… **Facilita** anÃ¡lises estatÃ­sticas e qualitativas
- âœ… **Economiza tempo** para pesquisas acadÃªmicas

---

## ğŸ—ï¸ Como o CÃ³digo Funciona (Arquitetura Detalhada)

### ğŸ§  **VisÃ£o Geral da Arquitetura**

O sistema Ã© composto por **mÃ³dulos especializados** que trabalham em conjunto para coletar, processar e analisar dados:

```
ğŸŒ UNA-SUS Website
    â†“
ğŸ” Scraper Principal (scraper_unasus_melhorado.py)
    â†“
ğŸ“Š Processadores Especializados
    â†“
ğŸ’¾ Sistema de PersistÃªncia
    â†“
ğŸ“ˆ AnÃ¡lise DEIA
    â†“
ğŸ“ Arquivo CSV Final
```

### ğŸ”§ **Componentes Principais**

#### 1ï¸âƒ£ **MÃ³dulo de ConexÃ£o e AutenticaÃ§Ã£o**
```python
# ğŸŒ ConfiguraÃ§Ãµes de rede
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
}

COOKIES = {
    "JSESSIONID": "sessÃ£o_ativa",
    "BIGipServer": "balanceamento_de_carga"
}

# ğŸ“‹ Payload para requisiÃ§Ãµes POST
PAYLOAD_INICIAL = {
    "draw": "1",
    "columns[0][data]": "0",
    "columns[0][name]": "",
    "columns[0][searchable]": "true",
    "columns[0][orderable]": "true",
    "start": "0",
    "length": "10",
    "search[value]": "",
    "search[regex]": "false"
}
```

**ğŸ¯ FunÃ§Ã£o**: Estabelece conexÃ£o segura com o servidor UNA-SUS, simulando um navegador real.

#### 2ï¸âƒ£ **MÃ³dulo de PaginaÃ§Ã£o Inteligente**
```python
def processar_pagina(pagina, logger):
    """
    ğŸ“„ Processa uma pÃ¡gina de resultados da UNA-SUS
    
    ğŸ”„ Fluxo:
    1. Faz requisiÃ§Ã£o POST com parÃ¢metros de paginaÃ§Ã£o
    2. Extrai dados JSON da resposta
    3. Processa cada curso encontrado
    4. Retorna dados estruturados
    """
    # ğŸ“Š ParÃ¢metros de paginaÃ§Ã£o
    payload = PAYLOAD_INICIAL.copy()
    payload["start"] = str(pagina * 10)
    
    # ğŸŒ RequisiÃ§Ã£o HTTP
    response = requests.post(URL, data=payload, headers=HEADERS, cookies=COOKIES)
    
    # ğŸ“‹ ExtraÃ§Ã£o de dados JSON
    dados = response.json()
    cursos = dados.get("data", [])
    
    return cursos
```

**ğŸ¯ FunÃ§Ã£o**: Navega pelas pÃ¡ginas de resultados, extraindo dados de forma sistemÃ¡tica.

#### 3ï¸âƒ£ **MÃ³dulo de ExtraÃ§Ã£o de Dados de Cursos**
```python
def extrair_dados_curso(curso, logger):
    """
    ğŸ“ Extrai dados completos de um curso individual
    
    ğŸ“Š Campos extraÃ­dos:
    - InformaÃ§Ãµes bÃ¡sicas (ID, nome, carga horÃ¡ria)
    - Dados organizacionais (Ã³rgÃ£o responsÃ¡vel)
    - CaracterÃ­sticas (formato, nÃ­vel, modalidade)
    - DescriÃ§Ã£o completa e palavras-chave
    - AnÃ¡lise DEIA inicial
    """
    # ğŸ†” Dados bÃ¡sicos
    co_seq_curso = curso[0]
    no_curso = curso[1]
    qt_carga_horaria_total = curso[2]
    
    # ğŸ¢ Dados organizacionais
    co_seq_orgao = curso[3]
    sg_orgao = curso[4]
    no_orgao = curso[5]
    
    # ğŸ“ DescriÃ§Ã£o completa (nova funcionalidade)
    ds_curso = extrair_descricao_curso(co_seq_curso, logger)
    
    # ğŸŒˆ AnÃ¡lise DEIA inicial
    tem_deia, deia_encontrado = analisar_deia_inicial(no_curso, ds_curso)
    
    return {
        "co_seq_curso": co_seq_curso,
        "no_curso": no_curso,
        "ds_curso": ds_curso,
        "tem_deia": tem_deia,
        "deia_encontrado": deia_encontrado,
        # ... outros campos
    }
```

**ğŸ¯ FunÃ§Ã£o**: Extrai e estrutura todos os dados de um curso individual.

#### 4ï¸âƒ£ **MÃ³dulo de ExtraÃ§Ã£o de DescriÃ§Ã£o Completa**
```python
def extrair_descricao_curso(id_curso, logger):
    """
    ğŸ“„ Extrai descriÃ§Ã£o completa da pÃ¡gina individual do curso
    
    ğŸ” EstratÃ©gias de extraÃ§Ã£o:
    1. Busca por seletores CSS especÃ­ficos
    2. Fallback para busca por texto
    3. Limpeza e formataÃ§Ã£o do conteÃºdo
    """
    url_curso = f"https://unasus.gov.br/cursos/{id_curso}"
    
    try:
        # ğŸŒ RequisiÃ§Ã£o Ã  pÃ¡gina do curso
        response = requests.get(url_curso, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ğŸ” Busca por descriÃ§Ã£o
        descricao = None
        
        # EstratÃ©gia 1: Seletor CSS especÃ­fico
        desc_element = soup.select_one('.curso-descricao, .descricao-curso, .content-description')
        if desc_element:
            descricao = desc_element.get_text(strip=True)
        
        # EstratÃ©gia 2: Busca por texto
        if not descricao:
            for element in soup.find_all(['p', 'div']):
                text = element.get_text(strip=True)
                if len(text) > 100 and 'curso' in text.lower():
                    descricao = text
                    break
        
        return descricao or ""
        
    except Exception as e:
        logger.warning(f"âŒ Erro ao extrair descriÃ§Ã£o do curso {id_curso}: {e}")
        return ""
```

**ğŸ¯ FunÃ§Ã£o**: Acessa pÃ¡ginas individuais dos cursos para extrair descriÃ§Ãµes completas.

#### 5ï¸âƒ£ **MÃ³dulo de Busca de Ofertas**
```python
def extrair_ofertas_do_curso(id_curso, logger):
    """
    ğŸ¯ Extrai todas as ofertas de um curso (ativas e encerradas)
    
    ğŸ” EstratÃ©gias:
    1. Busca ofertas ativas na pÃ¡gina principal
    2. Identifica links para ofertas encerradas
    3. Extrai dados de cada oferta individual
    """
    url_curso = f"https://unasus.gov.br/cursos/{id_curso}"
    
    try:
        response = requests.get(url_curso, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        ofertas_encontradas = []
        
        # ğŸ” Busca ofertas ativas
        links_ofertas = soup.find_all('a', href=re.compile(r'/oferta/\d+'))
        for link in links_ofertas:
            id_oferta = re.search(r'/oferta/(\d+)', link['href']).group(1)
            ofertas_encontradas.append(id_oferta)
        
        # ğŸ“‹ Busca ofertas encerradas
        ofertas_encerradas = buscar_ofertas_encerradas(soup, url_curso, logger)
        ofertas_encontradas.extend(ofertas_encerradas)
        
        return list(set(ofertas_encontradas))  # Remove duplicatas
        
    except Exception as e:
        logger.error(f"âŒ Erro ao buscar ofertas do curso {id_curso}: {e}")
        return []
```

**ğŸ¯ FunÃ§Ã£o**: Identifica e coleta todas as ofertas disponÃ­veis para um curso.

#### 6ï¸âƒ£ **MÃ³dulo de ExtraÃ§Ã£o de Dados de Ofertas**
```python
def extrair_dados_oferta(id_oferta, logger):
    """
    ğŸ“Š Extrai dados detalhados de uma oferta especÃ­fica
    
    ğŸ¯ EstratÃ©gia hÃ­brida:
    1. Tenta API REST primeiro (mais rÃ¡pido e preciso)
    2. Fallback para parsing HTML se API falhar
    3. ExtraÃ§Ã£o robusta de todos os campos
    """
    url_oferta = f"https://unasus.gov.br/oferta/{id_oferta}"
    url_api = f"https://unasus.gov.br/cursos/rest/oferta/{id_oferta}"
    
    dados_oferta = {}
    
    # ğŸš€ Tentativa 1: API REST
    try:
        api_headers = HEADERS.copy()
        api_headers.update({
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "X-Requested-With": "XMLHttpRequest",
            "Referer": url_oferta
        })
        
        response = requests.get(url_api, headers=api_headers, timeout=30)
        if response.status_code == 200:
            dados_json = response.json()
            
            # ğŸ“Š ExtraÃ§Ã£o de dados da API
            dados_oferta = {
                "vagas": dados_json.get("vagas", ""),
                "publico_alvo": dados_json.get("publico_alvo", ""),
                "local_oferta": dados_json.get("local_oferta", ""),
                "formato": dados_json.get("formato", ""),
                "programas_governo": dados_json.get("programas_governo", ""),
                "temas": dados_json.get("temas", ""),
                "decs": dados_json.get("decs", ""),
                "descricao_oferta": dados_json.get("descricao_oferta", ""),
                "palavras_chave": dados_json.get("palavras_chave", "")
            }
            
            logger.info("    âœ… Dados obtidos via API REST")
            return dados_oferta
            
    except Exception as e:
        logger.warning(f"    âš ï¸ API falhou, tentando HTML: {e}")
    
    # ğŸŒ Tentativa 2: Parsing HTML
    try:
        response = requests.get(url_oferta, headers=HEADERS, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ğŸ” ExtraÃ§Ã£o via HTML
        dados_oferta = extrair_dados_html_oferta(soup, logger)
        logger.info("    âœ… Dados obtidos via HTML")
        
    except Exception as e:
        logger.error(f"    âŒ Erro na extraÃ§Ã£o HTML: {e}")
    
    return dados_oferta
```

**ğŸ¯ FunÃ§Ã£o**: Extrai dados detalhados de ofertas usando API REST e fallback HTML.

#### 7ï¸âƒ£ **MÃ³dulo de AnÃ¡lise DEIA AvanÃ§ada**
```python
def encontrar_descritor_deia_melhorado(texto_completo):
    """
    ğŸŒˆ AnÃ¡lise avanÃ§ada de conteÃºdo DEIA
    
    ğŸ§  LÃ³gica:
    1. Combina todos os campos relevantes
    2. Busca por 150+ descritores diferentes
    3. Retorna o descritor mais especÃ­fico encontrado
    4. Prioriza descritores mais longos (mais especÃ­ficos)
    """
    # ğŸŒˆ Lista expandida de descritores DEIA (150+ termos)
    DESCRITORES_DEIA = [
        # ğŸ‘¥ PopulaÃ§Ãµes especÃ­ficas
        "SaÃºde da PopulaÃ§Ã£o Negra",
        "SaÃºde da PopulaÃ§Ã£o IndÃ­gena",
        "PopulaÃ§Ã£o LGBTQI+",
        "PopulaÃ§Ã£o Trans",
        "PopulaÃ§Ã£o em SituaÃ§Ã£o de Rua",
        
        # ğŸ¥ SaÃºde especÃ­fica
        "SaÃºde Mental",
        "SaÃºde da Mulher",
        "SaÃºde da CrianÃ§a",
        "SaÃºde do Idoso",
        
        # ğŸŒ Conceitos DEIA
        "Diversidade, Equidade e InclusÃ£o",
        "Diversidade, Equidade, InclusÃ£o e Acessibilidade",
        "InclusÃ£o, Diversidade, Equidade",
        
        # ğŸ“ EducaÃ§Ã£o inclusiva
        "EducaÃ§Ã£o Inclusiva",
        "EducaÃ§Ã£o Popular",
        "FormaÃ§Ã£o Continuada",
        
        # ... 150+ descritores
    ]
    
    # ğŸ” Busca pelo descritor mais especÃ­fico
    descritor_encontrado = None
    for descritor in sorted(DESCRITORES_DEIA, key=len, reverse=True):
        if descritor.lower() in texto_completo.lower():
            descritor_encontrado = descritor
            break
    
    return descritor_encontrado
```

**ğŸ¯ FunÃ§Ã£o**: Analisa texto completo em busca de conteÃºdo relacionado a DEIA.

#### 8ï¸âƒ£ **MÃ³dulo de Sistema de Checkpoint**
```python
def salvar_checkpoint(pagina_atual, cursos_processados, logger):
    """
    ğŸ’¾ Sistema de checkpoint para recuperaÃ§Ã£o automÃ¡tica
    
    ğŸ“Š Salva:
    - PÃ¡gina atual sendo processada
    - NÃºmero de cursos processados
    - Timestamp da Ãºltima execuÃ§Ã£o
    - Dados parciais coletados
    """
    checkpoint = {
        "pagina_atual": pagina_atual,
        "cursos_processados": cursos_processados,
        "timestamp": datetime.now().isoformat(),
        "status": "em_andamento"
    }
    
    with open("checkpoint.json", "w", encoding="utf-8") as f:
        json.dump(checkpoint, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ Progresso salvo: {cursos_processados} cursos processados")

def carregar_checkpoint():
    """
    ğŸ”„ Carrega checkpoint para retomar execuÃ§Ã£o
    """
    try:
        with open("checkpoint.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {"pagina_atual": 0, "cursos_processados": 0}
```

**ğŸ¯ FunÃ§Ã£o**: Permite retomar a execuÃ§Ã£o de onde parou em caso de interrupÃ§Ã£o.

### ğŸ”„ **Fluxo de ExecuÃ§Ã£o Detalhado**

#### ğŸ“‹ **Fase 1: InicializaÃ§Ã£o**
```python
def main():
    """
    ğŸš€ FunÃ§Ã£o principal - orquestra todo o processo
    """
    # ğŸ”§ ConfiguraÃ§Ã£o inicial
    logger = setup_logging()
    logger.info("ğŸš€ Iniciando scraper UNA-SUS melhorado")
    
    # ğŸ”„ Carrega checkpoint se existir
    checkpoint = carregar_checkpoint()
    pagina_atual = checkpoint["pagina_atual"]
    cursos_processados = checkpoint["cursos_processados"]
    
    # ğŸ“Š Inicializa estrutura de dados
    todos_dados = []
```

#### ğŸ“„ **Fase 2: Processamento de PÃ¡ginas**
```python
    # ğŸ”„ Loop principal de pÃ¡ginas
    while True:
        logger.info(f"=== PROCESSANDO PÃGINA {pagina_atual + 1} ===")
        
        # ğŸ“„ Extrai cursos da pÃ¡gina atual
        cursos_pagina = processar_pagina(pagina_atual, logger)
        
        if not cursos_pagina:
            logger.info("ğŸ“‹ Nenhum curso encontrado - fim dos dados")
            break
        
        # ğŸ“ Processa cada curso da pÃ¡gina
        for curso in cursos_pagina:
            dados_curso = extrair_dados_curso(curso, logger)
            ofertas = extrair_ofertas_do_curso(dados_curso["co_seq_curso"], logger)
            
            # ğŸ¯ Processa cada oferta
            for oferta in ofertas:
                dados_oferta = extrair_dados_oferta(oferta, logger)
                
                # ğŸ”— Combina dados do curso e da oferta
                registro_completo = {**dados_curso, **dados_oferta}
                todos_dados.append(registro_completo)
            
            cursos_processados += 1
```

#### ğŸ’¾ **Fase 3: PersistÃªncia e Checkpoint**
```python
        # ğŸ’¾ Salva progresso a cada lote
        if cursos_processados % 10 == 0:
            salvar_dados_parciais(todos_dados, logger)
            salvar_checkpoint(pagina_atual, cursos_processados, logger)
        
        pagina_atual += 1
```

#### ğŸ“Š **Fase 4: FinalizaÃ§Ã£o e RelatÃ³rio**
```python
    # ğŸ“Š Salva dados finais
    salvar_dados_finais(todos_dados, logger)
    
    # ğŸ“ˆ Gera relatÃ³rio final
    gerar_relatorio_final(todos_dados, logger)
    
    logger.info("ğŸ‰ Scraper finalizado com sucesso!")
```

### ğŸ›¡ï¸ **Sistema de Tratamento de Erros**

#### ğŸ”„ **RecuperaÃ§Ã£o AutomÃ¡tica**
```python
def executar_com_retry(funcao, max_tentativas=3, delay=30):
    """
    ğŸ”„ Executa funÃ§Ã£o com retry automÃ¡tico em caso de erro
    
    ğŸ¯ EstratÃ©gias:
    1. Tentativa imediata
    2. Retry com delay crescente
    3. Log detalhado de erros
    4. Fallback para mÃ©todos alternativos
    """
    for tentativa in range(max_tentativas):
        try:
            return funcao()
        except requests.exceptions.RequestException as e:
            if tentativa < max_tentativas - 1:
                logger.warning(f"âš ï¸ Tentativa {tentativa + 1} falhou: {e}")
                time.sleep(delay * (tentativa + 1))
            else:
                logger.error(f"âŒ Todas as tentativas falharam: {e}")
                raise
```

#### ğŸ“Š **ValidaÃ§Ã£o de Dados**
```python
def validar_dados_curso(dados):
    """
    âœ… Valida integridade dos dados coletados
    
    ğŸ” VerificaÃ§Ãµes:
    - Campos obrigatÃ³rios presentes
    - Tipos de dados corretos
    - Valores dentro de ranges esperados
    - ConsistÃªncia entre campos relacionados
    """
    campos_obrigatorios = ["co_seq_curso", "no_curso", "id_oferta"]
    
    for campo in campos_obrigatorios:
        if campo not in dados or not dados[campo]:
            return False, f"Campo obrigatÃ³rio ausente: {campo}"
    
    # ValidaÃ§Ãµes especÃ­ficas
    if dados.get("vagas") and not str(dados["vagas"]).isdigit():
        return False, "Vagas deve ser um nÃºmero"
    
    return True, "Dados vÃ¡lidos"
```

### ğŸ¯ **OtimizaÃ§Ãµes de Performance**

#### âš¡ **ConcorrÃªncia Controlada**
```python
# ğŸš¦ Controle de taxa de requisiÃ§Ãµes
time.sleep(1)  # Delay entre requisiÃ§Ãµes para nÃ£o sobrecarregar servidor

# ğŸ“Š Processamento em lotes
if len(todos_dados) % 10 == 0:
    salvar_dados_parciais(todos_dados, logger)
```

#### ğŸ’¾ **Gerenciamento de MemÃ³ria**
```python
# ğŸ§¹ Limpeza periÃ³dica de dados em memÃ³ria
if len(todos_dados) > 1000:
    salvar_dados_parciais(todos_dados, logger)
    todos_dados = []  # Libera memÃ³ria
```

#### ğŸ” **Cache Inteligente**
```python
# ğŸ“‹ Cache de requisiÃ§Ãµes para evitar duplicatas
cache_requisicoes = {}

def fazer_requisicao_cachead(url):
    if url in cache_requisicoes:
        return cache_requisicoes[url]
    
    response = requests.get(url, headers=HEADERS, timeout=30)
    cache_requisicoes[url] = response
    return response
```

---

## ğŸ“Š Dados Coletados

### ğŸ“ **InformaÃ§Ãµes dos Cursos**
| Campo | DescriÃ§Ã£o | Exemplo |
|-------|-----------|---------|
| `co_seq_curso` | ID Ãºnico do curso | `44538` |
| `no_curso` | Nome do curso | `"AtenÃ§Ã£o Ã  SaÃºde da PopulaÃ§Ã£o Negra"` |
| `qt_carga_horaria_total` | Carga horÃ¡ria | `60 horas` |
| `ds_curso` | DescriÃ§Ã£o completa | `"Curso focado em..."` |
| `palavras_chave_curso` | Palavras-chave | `"saÃºde, populaÃ§Ã£o negra, equidade"` |

### ğŸ¯ **InformaÃ§Ãµes das Ofertas**
| Campo | DescriÃ§Ã£o | Exemplo |
|-------|-----------|---------|
| `id_oferta` | ID da oferta | `416264` |
| `vagas` | NÃºmero de vagas | `1000` |
| `publico_alvo` | PÃºblico-alvo | `"Profissionais de saÃºde"` |
| `local_oferta` | Local | `"EAD"` |
| `formato` | Formato | `"Ensino a DistÃ¢ncia"` |

### ğŸŒˆ **AnÃ¡lise DEIA (Diversidade, Equidade, InclusÃ£o e Acessibilidade)**
| Campo | DescriÃ§Ã£o | Exemplo |
|-------|-----------|---------|
| `tem_deia` | Possui conteÃºdo DEIA | `"Sim"` ou `"NÃ£o"` |
| `deia_encontrado` | Descritor especÃ­fico | `"SaÃºde da PopulaÃ§Ã£o Negra"` |
| `texto_analisado_deia` | Texto analisado | `"TÃ­tulo: Curso... + DescriÃ§Ã£o:..."` |

---

## ğŸ› ï¸ Como Usar (Guia Passo a Passo)

### ğŸ“‹ **PrÃ©-requisitos**
Antes de comeÃ§ar, vocÃª precisa ter:
- **Python 3.8 ou superior** instalado
- **ConexÃ£o com a internet** estÃ¡vel
- **Conhecimento bÃ¡sico** de linha de comando

### ğŸ”§ **InstalaÃ§Ã£o**

#### 1ï¸âƒ£ **Clone o projeto**
```bash
git clone https://github.com/seu-usuario/una-sus.git
cd una-sus
```

#### 2ï¸âƒ£ **Instale as dependÃªncias**
```bash
pip install -r requirements.txt
```

#### 3ï¸âƒ£ **Verifique se tudo estÃ¡ funcionando**
```bash
python --version
pip list | grep -E "(requests|pandas|beautifulsoup)"
```

### ğŸš€ **ExecuÃ§Ã£o**

#### ğŸ¯ **OpÃ§Ã£o 1: Coleta Completa (Recomendado)**
```bash
python scraper_unasus_melhorado.py
```

**O que acontece:**
- ğŸ” **Busca** todos os cursos da UNA-SUS
- ğŸ“Š **Coleta** dados detalhados de cada curso
- ğŸŒˆ **Analisa** conteÃºdo DEIA automaticamente
- ğŸ’¾ **Salva** progresso a cada 10 cursos
- ğŸ“ˆ **Mostra** estatÃ­sticas em tempo real

#### ğŸ”„ **OpÃ§Ã£o 2: ReanÃ¡lise de Dados Existentes**
Se vocÃª jÃ¡ tem dados coletados e quer apenas atualizar a anÃ¡lise DEIA:
```bash
python reanalisar_deia_existente.py
```

#### ğŸ§ª **OpÃ§Ã£o 3: Teste da Busca DEIA**
Para validar se a busca DEIA estÃ¡ funcionando:
```bash
python testar_busca_deia.py
```

---

## ğŸ“ˆ Monitoramento e Logs

### ğŸ‘€ **Como Acompanhar o Progresso**

Durante a execuÃ§Ã£o, vocÃª verÃ¡ mensagens como:
```
2025-07-26 15:30:15 - INFO - === PROCESSANDO PÃGINA 5 ===
2025-07-26 15:30:16 - INFO - Processando curso 44538: AtenÃ§Ã£o Ã  SaÃºde da PopulaÃ§Ã£o Negra
2025-07-26 15:30:16 - INFO - ğŸŒˆ DEIA encontrado no curso 44538: PopulaÃ§Ã£o Negra
2025-07-26 15:30:17 - INFO - Buscando ofertas do curso 44538...
2025-07-26 15:30:17 - INFO -   âœ… Oferta encontrada: 416264
2025-07-26 15:30:18 - INFO -   ğŸ” Extraindo dados da oferta 416264...
2025-07-26 15:30:18 - INFO -     âœ… Dados obtidos via API REST
2025-07-26 15:30:18 - INFO -     âœ… Vagas extraÃ­das: 1000
```

### ğŸ¨ **Significado dos Emojis nos Logs**

| Emoji | Significado | Exemplo |
|-------|-------------|---------|
| ğŸŒˆ | **DEIA detectado** | `ğŸŒˆ DEIA encontrado: PopulaÃ§Ã£o Negra` |
| âœ… | **Sucesso** | `âœ… Oferta encontrada: 416264` |
| ğŸ” | **Processando** | `ğŸ” Extraindo dados...` |
| ğŸ“Š | **EstatÃ­sticas** | `ğŸ“Š Total de registros: 1656` |
| ğŸ’¾ | **Salvando** | `ğŸ’¾ Progresso salvo` |
| âš ï¸ | **Aviso** | `âš ï¸ Curso sem ofertas` |
| âŒ | **Erro** | `âŒ Erro na conexÃ£o` |

### ğŸ“Š **EstatÃ­sticas Finais**
Ao final da execuÃ§Ã£o, vocÃª verÃ¡:
```
=== RELATÃ“RIO FINAL ===
ğŸ“Š Total de registros: 1656
ğŸŒˆ Cursos com DEIA: 604 (36.5%)
ğŸ“š Cursos sem DEIA: 1052 (63.5%)
ğŸ“ Arquivo salvo: unasus_ofertas_melhoradas.csv
```

---

## ğŸ“ Estrutura do Projeto

```
una-sus/
â”œâ”€â”€ ğŸ scraper_unasus_melhorado.py    # Scraper principal (MAIS RECENTE)
â”œâ”€â”€ ğŸ scraper_unasus.py              # Scraper original (referÃªncia)
â”œâ”€â”€ ğŸ reanalisar_deia_existente.py   # ReanÃ¡lise DEIA
â”œâ”€â”€ ğŸ testar_busca_deia.py           # Testes DEIA
â”œâ”€â”€ ğŸ“Š unasus_ofertas_melhoradas.csv  # Dados coletados
â”œâ”€â”€ ğŸ“š README.md                      # Esta documentaÃ§Ã£o
â”œâ”€â”€ ğŸ“š README_MELHORIAS_DEIA.md       # Detalhes das melhorias
â”œâ”€â”€ ğŸ“‹ requirements.txt               # DependÃªncias
â”œâ”€â”€ âš™ï¸ pyproject.toml                 # ConfiguraÃ§Ã£o do projeto
â”œâ”€â”€ ğŸ³ Dockerfile                     # ContainerizaÃ§Ã£o
â”œâ”€â”€ ğŸ³ docker-compose.yml             # OrquestraÃ§Ã£o
â””â”€â”€ ğŸ“„ LICENSE                        # LicenÃ§a MIT
```

---

## ğŸŒˆ AnÃ¡lise DEIA - ExplicaÃ§Ã£o Detalhada

### ğŸ¯ **O que Ã© DEIA?**
**DEIA** significa **Diversidade, Equidade, InclusÃ£o e Acessibilidade**. Ã‰ um conceito fundamental para:
- ğŸ¥ **SaÃºde pÃºblica** inclusiva
- ğŸ“ **EducaÃ§Ã£o** acessÃ­vel a todos
- ğŸ¤ **Sociedade** mais justa e equitativa

### ğŸ” **Como Funciona a DetecÃ§Ã£o?**

O sistema analisa **todos os campos** coletados:
1. **TÃ­tulo do curso** (`no_curso`)
2. **DescriÃ§Ã£o do curso** (`ds_curso`)
3. **DescriÃ§Ã£o da oferta** (`descricao_oferta`)
4. **Palavras-chave** (`palavras_chave`)
5. **PÃºblico-alvo** (`publico_alvo`)
6. **Temas** (`temas`)
7. **DeCs** (`decs`)
8. **Programas de governo** (`programas_governo`)
9. **Texto da pÃ¡gina inicial** (`texto_pagina_inicial`)

### ğŸ“ **Exemplos de Descritores DEIA**

#### ğŸ‘¥ **PopulaÃ§Ãµes EspecÃ­ficas**
- PopulaÃ§Ã£o Negra, PopulaÃ§Ã£o IndÃ­gena
- PopulaÃ§Ã£o LGBTQI+, Trans, TransgÃªnero
- PopulaÃ§Ã£o em SituaÃ§Ã£o de Rua
- PopulaÃ§Ã£o Privada de Liberdade

#### ğŸ¥ **SaÃºde EspecÃ­fica**
- SaÃºde Mental, SaÃºde da Mulher
- SaÃºde da CrianÃ§a, SaÃºde do Idoso
- SaÃºde da PopulaÃ§Ã£o Negra
- SaÃºde IndÃ­gena

#### ğŸŒ **Conceitos DEIA**
- Diversidade, Equidade, InclusÃ£o
- Acessibilidade, Pertencimento
- Direitos Humanos, Cidadania
- Vulnerabilidade, DiscriminaÃ§Ã£o

---

## ğŸš¨ SoluÃ§Ã£o de Problemas

### â“ **Perguntas Frequentes**

#### **Q: O scraper parou no meio. O que fazer?**
**A:** NÃ£o se preocupe! O sistema tem **checkpoint automÃ¡tico**. Basta executar novamente:
```bash
python scraper_unasus_melhorado.py
```
Ele continuarÃ¡ de onde parou.

#### **Q: NÃ£o encontrou nenhum curso com DEIA. Ã‰ normal?**
**A:** Se vocÃª estÃ¡ usando o scraper original (`scraper_unasus.py`), sim. Use o **melhorado**:
```bash
python scraper_unasus_melhorado.py
```

#### **Q: O arquivo CSV estÃ¡ muito grande. Ã‰ normal?**
**A:** Sim! O arquivo pode ter **8+ MB** porque contÃ©m:
- 1.656 registros de ofertas
- 30 colunas de dados
- Textos completos dos cursos

#### **Q: Como saber se estÃ¡ funcionando?**
**A:** Observe os logs:
- âœ… **Verde** = funcionando
- âš ï¸ **Amarelo** = aviso (normal)
- âŒ **Vermelho** = erro (raro)

### ğŸ”§ **Problemas TÃ©cnicos**

#### **Erro de ConexÃ£o**
```bash
# Tente novamente em alguns minutos
python scraper_unasus_melhorado.py
```

#### **Erro de DependÃªncias**
```bash
# Reinstale as dependÃªncias
pip install -r requirements.txt --force-reinstall
```

#### **Erro de PermissÃ£o**
```bash
# No Windows, execute como administrador
# No Linux/Mac, use sudo se necessÃ¡rio
```

---

## ğŸ“Š AnÃ¡lise dos Resultados

### ğŸ“ˆ **EstatÃ­sticas TÃ­picas**
- **Total de cursos**: ~550 cursos Ãºnicos
- **Total de ofertas**: ~1.650 ofertas
- **Cursos com DEIA**: ~600 (36%)
- **Cursos sem DEIA**: ~1.050 (64%)

### ğŸ” **Como Analisar os Dados**

#### **1. Abrir no Excel/LibreOffice**
```bash
# O arquivo Ã© compatÃ­vel com Excel
unasus_ofertas_melhoradas.csv
```

#### **2. Filtrar por DEIA**
- Coluna `tem_deia` = "Sim"
- Ver coluna `deia_encontrado` para detalhes

#### **3. AnÃ¡lise por Tema**
- Coluna `temas` para categorizaÃ§Ã£o
- Coluna `decs` para classificaÃ§Ã£o mÃ©dica

#### **4. AnÃ¡lise GeogrÃ¡fica**
- Coluna `local_oferta` para distribuiÃ§Ã£o
- Coluna `no_orgao` para instituiÃ§Ãµes

---

## ğŸ¤ ContribuiÃ§Ã£o

### ğŸ’¡ **Como Contribuir**

1. **ğŸ” Encontrou um bug?**
   - Abra uma **Issue** no GitHub
   - Descreva o problema detalhadamente

2. **âœ¨ Tem uma ideia?**
   - Sugira melhorias via **Issue**
   - Proponha novos descritores DEIA

3. **ğŸ’» Quer programar?**
   - FaÃ§a um **Fork** do projeto
   - Crie uma **Pull Request**

### ğŸ“ **PadrÃµes do Projeto**

#### **Emojis Educativos**
Use emojis para facilitar a compreensÃ£o:
- ğŸ“ = EducaÃ§Ã£o/Academia
- ğŸ¥ = SaÃºde
- ğŸŒˆ = DEIA/Diversidade
- ğŸ“Š = Dados/EstatÃ­sticas
- ğŸ” = Busca/AnÃ¡lise
- âœ… = Sucesso
- âŒ = Erro

#### **ComentÃ¡rios no CÃ³digo**
```python
# ğŸŒˆ Busca por descritores DEIA no texto
def encontrar_descritor_deia_melhorado(texto_completo):
    """
    ğŸ“ Analisa texto completo em busca de descritores DEIA
    
    Args:
        texto_completo (str): Texto combinado de todos os campos
        
    Returns:
        str: Descritor DEIA encontrado ou None
    """
```

---

## ğŸ“š Recursos Educativos

### ğŸ“ **Para Estudantes**
- **Web Scraping**: Aprenda a coletar dados da web
- **AnÃ¡lise de Dados**: Trabalhe com datasets reais
- **Python**: Pratique programaÃ§Ã£o em Python
- **SaÃºde PÃºblica**: Entenda dados educacionais em saÃºde

### ğŸ”¬ **Para Pesquisadores**
- **Metodologia**: Exemplo de coleta sistemÃ¡tica de dados
- **DEIA**: Framework para anÃ¡lise de diversidade
- **EstatÃ­sticas**: Dados quantitativos para pesquisas
- **Qualidade**: Dados validados e estruturados

### ğŸ’» **Para Desenvolvedores**
- **Clean Code**: CÃ³digo bem estruturado e documentado
- **Error Handling**: Tratamento robusto de erros
- **Logging**: Sistema de logs detalhado
- **ModularizaÃ§Ã£o**: FunÃ§Ãµes bem separadas

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a **LicenÃ§a MIT** - veja o arquivo [LICENSE](LICENSE) para detalhes.

**O que isso significa?**
- âœ… **Pode usar** livremente
- âœ… **Pode modificar** o cÃ³digo
- âœ… **Pode distribuir** o projeto
- âœ… **Pode usar comercialmente**
- âŒ **NÃ£o precisa** dar crÃ©dito (mas Ã© apreciado!)

---

## ğŸ™ Agradecimentos

- ğŸ¥ **UNA-SUS** pela disponibilizaÃ§Ã£o dos dados
- ğŸ **Comunidade Python** pelos recursos utilizados
- ğŸŒˆ **Comunidade DEIA** pela inspiraÃ§Ã£o
- ğŸ‘¥ **Contribuidores** do projeto

---

## ğŸ“ Suporte

### ğŸ†˜ **Precisa de Ajuda?**

1. **ğŸ“– Leia** esta documentaÃ§Ã£o completa
2. **ğŸ” Verifique** as seÃ§Ãµes de soluÃ§Ã£o de problemas
3. **ğŸ’¬ Abra** uma Issue no GitHub
4. **ğŸ“§ Entre em contato** com os mantenedores

### ğŸ“š **Recursos Adicionais**

- ğŸ“– **DocumentaÃ§Ã£o Python**: https://docs.python.org/
- ğŸ **Tutorial Requests**: https://requests.readthedocs.io/
- ğŸ² **BeautifulSoup**: https://www.crummy.com/software/BeautifulSoup/
- ğŸ“Š **Pandas**: https://pandas.pydata.org/

---

**ğŸ¯ Desenvolvido com â¤ï¸ para facilitar pesquisas em saÃºde pÃºblica e promover educaÃ§Ã£o inclusiva!**

*Ãšltima atualizaÃ§Ã£o: Julho 2025*
