# üîç An√°lise Comparativa: Scraper Original vs. Modelo Modular

## üìã Vis√£o Geral da Compara√ß√£o

Esta an√°lise compara o **`scraper_unasus.py`** (vers√£o original robusta) com o **sistema modular de coleta** implementado na pasta `Grounded Theory`, especificamente o **`coletor_unasus_completo.py`**.

---

## üéØ Objetivos da Compara√ß√£o

### **Scraper Original (`scraper_unasus.py`)**
- ‚úÖ **Coleta b√°sica** de dados UNA-SUS
- ‚úÖ **An√°lise DEIA integrada** durante a coleta
- ‚úÖ **Extra√ß√£o de ofertas** detalhadas
- ‚úÖ **Salvamento incremental** em CSV
- ‚úÖ **Controle de duplicatas** por ID

### **Sistema Modular (`coletor_unasus_completo.py`)**
- ‚úÖ **Coleta completa** sem filtros
- ‚úÖ **Separa√ß√£o** entre coleta e an√°lise
- ‚úÖ **Database fiel** preservado
- ‚úÖ **Sistema robusto** de logging
- ‚úÖ **Checkpointing avan√ßado**

---

## üèóÔ∏è Arquitetura Comparativa

### **1. Estrutura de C√≥digo**

#### **A. Scraper Original (Monol√≠tico)**
```python
# Estrutura monol√≠tica - tudo em um arquivo
‚îú‚îÄ‚îÄ Configura√ß√µes (URL, headers, cookies, payload)
‚îú‚îÄ‚îÄ Fun√ß√µes de an√°lise DEIA
‚îú‚îÄ‚îÄ Fun√ß√µes de extra√ß√£o de ofertas
‚îú‚îÄ‚îÄ Loop principal de coleta
‚îî‚îÄ‚îÄ Salvamento incremental
```

**Caracter√≠sticas:**
- üìÅ **1 arquivo** com todas as funcionalidades
- üîÑ **Coleta + An√°lise** integradas
- üìä **Filtros DEIA** aplicados durante coleta
- üíæ **Salvamento direto** em CSV

#### **B. Sistema Modular (Separado)**
```python
# Estrutura modular - separa√ß√£o de responsabilidades
‚îú‚îÄ‚îÄ ColetorUnasusCompleto (coleta pura)
‚îú‚îÄ‚îÄ ProcessadorDEIA (an√°lise separada)
‚îú‚îÄ‚îÄ AnalisadorGeral (an√°lises flex√≠veis)
‚îî‚îÄ‚îÄ Orquestrador (coordena√ß√£o)
```

**Caracter√≠sticas:**
- üìÅ **M√∫ltiplos m√≥dulos** especializados
- üîÑ **Coleta independente** da an√°lise
- üìä **An√°lises n√£o-destrutivas**
- üíæ **M√∫ltiplos formatos** de sa√≠da

---

## üîß Compara√ß√£o T√©cnica Detalhada

### **1. Configura√ß√µes de Rede**

#### **A. Scraper Original**
```python
# Configura√ß√µes b√°sicas
url = "https://www.unasus.gov.br/cursos/rest/busca"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "X-Requested-With": "XMLHttpRequest",
    "Origin": "https://www.unasus.gov.br",
    "Referer": "https://www.unasus.gov.br/cursos/busca?status=todos&busca=&ordenacao=Relev%C3%A2ncia%20na%20busca"
}

cookies = {
    "PORTAL_UNASUS": "4ru34cs848mfbopb6vseqluni4",
    "UNASUSAnonID": "ID1ef7d6246158f7cf31c06b928bc56f8e",
    "_shibsession_64656661756c7468747470733a2f2f7777772e756e617375732e676f762e6272": "_329a72cffc11d2904ae393c82d0cfb72"
}

payload = {"busca": "", "ordenacao": "Por nome", "status": "Todos", "proximo": 0}
```

#### **B. Sistema Modular**
```python
# Configura√ß√µes id√™nticas (baseadas no backup original)
self.url_base = "https://www.unasus.gov.br/cursos/rest/busca"
self.headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "X-Requested-With": "XMLHttpRequest",
    "Origin": "https://www.unasus.gov.br",
    "Referer": "https://www.unasus.gov.br/cursos/busca?status=todos&busca=&ordenacao=Relev%C3%A2ncia%20na%20busca"
}

self.cookies = {
    "PORTAL_UNASUS": "4ru34cs848mfbopb6vseqluni4",
    "UNASUSAnonID": "ID1ef7d6246158f7cf31c06b928bc56f8e",
    "_shibsession_64656661756c7468747470733a2f2f7777772e756e617375732e676f762e6272": "_329a72cffc11d2904ae393c82d0cfb72"
}

self.payload = {
    "busca": "",
    "ordenacao": "Por nome", 
    "status": "Todos",
    "proximo": 0
}
```

**‚úÖ Conclus√£o**: Configura√ß√µes **id√™nticas** - ambos usam o mesmo modelo de requisi√ß√£o.

### **2. Descritores DEIA**

#### **A. Scraper Original (Limitado)**
```python
descritores = [
    "Diversidade, Equidade e Integra√ß√£o",
    "Diversidade, Equidade, Inclus√£o e Pertencimento",
    "Diversidade, Equidade, Inclus√£o, Acessibilidade",
    "Diversidade, Igualdade e Inclus√£o",
    "Diversidade, Igualdade, Inclus√£o e Acessibilidade",
    "Diversidade, Igualdade, Inclus√£o, Pertencimento",
    "Equidade, Diversidade e Inclus√£o",
    "Inclus√£o, Diversidade, Equidade e Acessibilidade",
    "Inclus√£o, Diversidade, Equidade, Acessibilidade"
]
```

**Caracter√≠sticas:**
- üìù **10 descritores** espec√≠ficos
- üéØ **Foco em frases completas**
- üîç **Aplicado durante coleta**

#### **B. Sistema Modular (Expandido)**
```python
DESCRITORES_DEIA = {
    "diversidade": [
        "diversidade", "diverso", "pluralidade", "multicultural",
        "intercultural", "transcultural", "multirracial", "heterogeneidade"
    ],
    "equidade": [
        "equidade", "equitativo", "justi√ßa social", "igualdade",
        "paridade", "equilibrio", "redistribui√ß√£o", "justi√ßa"
    ],
    "inclus√£o": [
        "inclus√£o", "inclusivo", "acolhimento", "integra√ß√£o",
        "participa√ß√£o", "pertencimento", "comunidade", "aceita√ß√£o"
    ],
    "acessibilidade": [
        "acessibilidade", "acess√≠vel", "acesso", "barreiras",
        "adapta√ß√£o", "tecnologia assistiva", "design universal"
    ],
    "popula√ß√µes_espec√≠ficas": [
        "popula√ß√£o negra", "ind√≠gena", "quilombola", "ribeirinha",
        "pessoas com defici√™ncia", "LGBTQIA+", "mulheres", "idosos"
    ]
}
```

**Caracter√≠sticas:**
- üìù **50+ descritores** organizados por categoria
- üéØ **Foco em palavras-chave**
- üîç **Aplicado ap√≥s coleta** (n√£o-destrutivo)

### **3. Processamento de Dados**

#### **A. Scraper Original**
```python
def encontrar_descritor(titulo, descricao, descritores):
    """Encontra descritores DEIA no t√≠tulo e descri√ß√£o do curso."""
    texto = (titulo or "") + " " + (descricao or "")
    for descritor in descritores:
        if descritor.lower() in texto.lower():
            return descritor
    return ""

# Aplica√ß√£o durante coleta
encontrado = encontrar_descritor(titulo, descricao, descritores)
curso["tem_deia"] = "Sim" if encontrado else "N√£o"
curso["deia_encontrado"] = encontrado
```

**Caracter√≠sticas:**
- üîç **An√°lise simples** (busca exata)
- üìä **Integrada** na coleta
- üéØ **Apenas t√≠tulo e descri√ß√£o**

#### **B. Sistema Modular**
```python
def _identificar_elementos_deia(self, texto: str) -> List[Dict]:
    """Identifica elementos DEIA com contexto."""
    elementos_encontrados = []
    
    for categoria, descritores in self.descritores_deia.items():
        for descritor in descritores:
            if descritor.lower() in texto.lower():
                # Encontrar contexto
                contexto = self._extrair_contexto(texto, descritor)
                elementos_encontrados.append({
                    "categoria": categoria,
                    "descritor": descritor,
                    "contexto": contexto,
                    "campo": campo_origem
                })
    
    return elementos_encontrados
```

**Caracter√≠sticas:**
- üîç **An√°lise avan√ßada** com contexto
- üìä **Separada** da coleta
- üéØ **M√∫ltiplos campos** analisados

### **4. Extra√ß√£o de Ofertas**

#### **A. Scraper Original (Detalhada)**
```python
def extrair_ofertas_do_curso(id_curso):
    """Extrai ofertas de um curso espec√≠fico com logs detalhados."""
    url_curso = f"https://www.unasus.gov.br/cursos/curso/{id_curso}"
    
    # Busca ofertas ativas
    for link in soup.find_all("a", href=True):
        if any(pattern in href for pattern in ["/cursos/oferta/", "../oferta/", "oferta/"]):
            id_oferta = href.split("/")[-1]
            if id_oferta.isdigit():
                ofertas.append(id_oferta)
    
    # Busca ofertas encerradas
    botoes_encerradas = soup.find_all("a", string=lambda t: t and "encerrada" in t.lower())
    
    return ofertas

def extrair_dados_oferta(id_oferta):
    """Extrai dados detalhados de uma oferta."""
    # Extra√ß√£o de 20+ campos espec√≠ficos
    dados = {
        "id_oferta": id_oferta,
        "titulo_oferta": "",
        "carga_horaria": "",
        "vagas": "",
        "inscricoes_abertas": "",
        "data_inicio": "",
        "data_fim": "",
        "coordenador": "",
        "tutores": "",
        "certificacao": "",
        "pre_requisitos": "",
        "objetivos": "",
        "metodologia": "",
        "avaliacao": "",
        "bibliografia": "",
        "temas": "",
        "decs": "",
        "descricao_oferta": "",
        "palavras_chave": ""
    }
```

**Caracter√≠sticas:**
- üîç **Extra√ß√£o detalhada** de ofertas
- üìä **20+ campos** espec√≠ficos
- üéØ **Foco em ofertas** individuais

#### **B. Sistema Modular (Simplificada)**
```python
def _processar_curso_completo(self, curso: Dict) -> Dict:
    """Processa um curso mantendo TODOS os dados originais."""
    curso_processado = curso.copy()
    
    # Adicionar metadados de coleta
    curso_processado["metadata_coleta"] = {
        "timestamp_coleta": datetime.now().isoformat(),
        "pagina_coleta": self.pagina_atual,
        "versao_coletor": "1.0.0",
        "tipo_coleta": "completa_sem_filtros"
    }
    
    # Garantir campos obrigat√≥rios
    campos_obrigatorios = [
        "id", "titulo", "descricao", "carga_horaria", "status",
        "categoria", "publico_alvo", "palavras_chave", "link",
        "vagas", "numero_vagas", "qt_vagas", "vagas_disponiveis",
        "inicio_inscricao", "fim_inscricao", "data_inicio", "data_fim",
        "modalidade", "tipo_curso", "nivel", "area_tematica",
        "instituicao", "coordenador", "tutores", "certificacao",
        "pre_requisitos", "objetivos", "metodologia", "avaliacao",
        "bibliografia"
    ]
```

**Caracter√≠sticas:**
- üîç **Preserva√ß√£o completa** dos dados originais
- üìä **Metadados** de coleta
- üéØ **Foco na integridade** dos dados

### **5. Salvamento de Dados**

#### **A. Scraper Original**
```python
# Salvamento incremental
csv_path = "unasus_ofertas_detalhadas.csv"
lote = 10  # Salva a cada 10 cursos

if len(todos_detalhes) >= lote:
    if os.path.exists(csv_path):
        df_existente = pd.read_csv(csv_path, encoding="utf-8-sig")
        df_novo = pd.DataFrame(todos_detalhes)
        df_final = pd.concat([df_existente, df_novo], ignore_index=True)
    else:
        df_final = pd.DataFrame(todos_detalhes)
    df_final.to_csv(csv_path, index=False, encoding="utf-8-sig")
```

**Caracter√≠sticas:**
- üíæ **Apenas CSV**
- üîÑ **Salvamento incremental**
- üìä **Controle de duplicatas**

#### **B. Sistema Modular**
```python
def _salvar_dados_completos(self):
    """Salva dados em m√∫ltiplos formatos."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Salvar em JSON
    json_path = f"dados/unasus_dados_completos_{timestamp}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(self.dados_coletados, f, ensure_ascii=False, indent=2)
    
    # Salvar em CSV
    csv_path = f"dados/unasus_dados_completos_{timestamp}.csv"
    df = pd.DataFrame(self.dados_coletados)
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    
    # Salvar em Excel (opcional)
    try:
        excel_path = f"dados/unasus_dados_completos_{timestamp}.xlsx"
        df.to_excel(excel_path, index=False)
    except ImportError:
        self.logger.info("openpyxl n√£o instalado. Pulando salvamento Excel.")
```

**Caracter√≠sticas:**
- üíæ **M√∫ltiplos formatos** (JSON, CSV, Excel)
- üîÑ **Checkpointing robusto**
- üìä **Logs detalhados**

### **6. Logging e Monitoramento**

#### **A. Scraper Original**
```python
# Logs b√°sicos com print
print(f"Buscando ofertas do curso {id_curso}...")
print(f"  ‚úÖ Oferta encontrada: {id_oferta}")
print(f"Progresso salvo ap√≥s {len(cursos_processados)} cursos")
print(f"P√°gina {pagina} processada.")
```

**Caracter√≠sticas:**
- üìù **Logs b√°sicos** com print
- üîÑ **Sem persist√™ncia** de logs
- üìä **Informa√ß√µes limitadas**

#### **B. Sistema Modular**
```python
def _configurar_logger(self) -> logging.Logger:
    """Configura o logger para o coletor."""
    os.makedirs("logs", exist_ok=True)
    
    logger = logging.getLogger("ColetorUnasusCompleto")
    logger.setLevel(logging.INFO)
    
    # Handler para arquivo
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    fh = logging.FileHandler(f"logs/coletor_unasus_{timestamp}.log", encoding="utf-8")
    
    # Handler para console
    ch = logging.StreamHandler()
    
    # Formato
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger
```

**Caracter√≠sticas:**
- üìù **Logs estruturados** com logging
- üîÑ **Persist√™ncia** em arquivos
- üìä **Informa√ß√µes detalhadas**

---

## üìä Compara√ß√£o de Funcionalidades

### **1. Escopo de Coleta**

| Aspecto | Scraper Original | Sistema Modular |
|---------|------------------|-----------------|
| **Dados Coletados** | Cursos + Ofertas | Apenas Cursos |
| **Campos Extra√≠dos** | 20+ campos espec√≠ficos | Todos os campos originais |
| **Integridade** | Processamento durante coleta | Preserva√ß√£o completa |
| **Filtros** | DEIA aplicado durante coleta | Sem filtros na coleta |

### **2. An√°lise DEIA**

| Aspecto | Scraper Original | Sistema Modular |
|---------|------------------|-----------------|
| **Descritores** | 10 frases espec√≠ficas | 50+ palavras-chave |
| **Aplica√ß√£o** | Durante coleta | Ap√≥s coleta |
| **Campos Analisados** | T√≠tulo + Descri√ß√£o | M√∫ltiplos campos |
| **Contexto** | N√£o | Sim |
| **Categoriza√ß√£o** | Sim/N√£o | Categorias detalhadas |

### **3. Persist√™ncia de Dados**

| Aspecto | Scraper Original | Sistema Modular |
|---------|------------------|-----------------|
| **Formatos** | CSV apenas | JSON, CSV, Excel |
| **Checkpointing** | B√°sico | Robusto |
| **Controle de Duplicatas** | Por ID de curso | Por ID de curso |
| **Metadados** | N√£o | Sim |

### **4. Monitoramento**

| Aspecto | Scraper Original | Sistema Modular |
|---------|------------------|-----------------|
| **Logs** | Print b√°sico | Logging estruturado |
| **Persist√™ncia** | N√£o | Sim |
| **N√≠veis** | Apenas info | Debug, Info, Warning, Error |
| **Relat√≥rios** | N√£o | Sim |

---

## üéØ Vantagens e Desvantagens

### **Scraper Original**

#### **‚úÖ Vantagens:**
- üöÄ **Simplicidade**: C√≥digo monol√≠tico f√°cil de entender
- üîç **Detalhamento**: Extra√ß√£o detalhada de ofertas
- üìä **An√°lise integrada**: DEIA aplicado durante coleta
- üíæ **Salvamento direto**: CSV pronto para uso

#### **‚ùå Desvantagens:**
- üîÑ **Acoplamento**: Coleta e an√°lise misturadas
- üìä **Filtros**: Dados filtrados durante coleta
- üíæ **Formato √∫nico**: Apenas CSV
- üìù **Logs limitados**: Sem persist√™ncia

### **Sistema Modular**

#### **‚úÖ Vantagens:**
- üß© **Modularidade**: Separa√ß√£o clara de responsabilidades
- üìä **Integridade**: Database fiel preservado
- üîç **Flexibilidade**: An√°lises n√£o-destrutivas
- üíæ **M√∫ltiplos formatos**: JSON, CSV, Excel
- üìù **Logs robustos**: Persist√™ncia e estrutura√ß√£o
- üîÑ **Checkpointing**: Recupera√ß√£o de falhas

#### **‚ùå Desvantagens:**
- üèóÔ∏è **Complexidade**: M√∫ltiplos m√≥dulos
- üîç **Escopo limitado**: Sem extra√ß√£o de ofertas
- üìö **Curva de aprendizado**: Mais arquivos para entender

---

## üîÑ Fluxos de Trabalho Comparativos

### **1. Scraper Original**
```
üåê Conex√£o UNA-SUS
    ‚Üì
üìÑ Pagina√ß√£o autom√°tica
    ‚Üì
üîç Extra√ß√£o de cursos
    ‚Üì
üìä An√°lise DEIA (durante coleta)
    ‚Üì
üîç Extra√ß√£o de ofertas
    ‚Üì
üíæ Salvamento CSV incremental
```

### **2. Sistema Modular**
```
üåê Conex√£o UNA-SUS
    ‚Üì
üìÑ Pagina√ß√£o autom√°tica
    ‚Üì
üîç Coleta completa (sem filtros)
    ‚Üì
üíæ Salvamento m√∫ltiplos formatos
    ‚Üì
üìä An√°lise DEIA (separada)
    ‚Üì
üìà An√°lise geral (separada)
```

---

## üéØ Recomenda√ß√µes de Uso

### **Use Scraper Original quando:**
- üéØ **Precisa de ofertas detalhadas**
- üöÄ **Quer simplicidade**
- üìä **An√°lise DEIA b√°sica √© suficiente**
- üíæ **CSV √© o formato desejado**

### **Use Sistema Modular quando:**
- üéØ **Precisa de database fiel**
- üîç **Quer an√°lises flex√≠veis**
- üìä **Pesquisa acad√™mica rigorosa**
- üíæ **M√∫ltiplos formatos de sa√≠da**
- üìù **Logs detalhados necess√°rios**

---

## üîÆ Evolu√ß√£o e Integra√ß√£o

### **Poss√≠vel Integra√ß√£o:**
```python
# Combinar o melhor dos dois mundos
class ScraperUnasusIntegrado:
    def __init__(self):
        self.coletor = ColetorUnasusCompleto()  # Coleta fiel
        self.extrator_ofertas = ExtratorOfertas()  # Extra√ß√£o detalhada
        self.analisador_deia = ProcessadorDEIA()  # An√°lise avan√ßada
    
    def executar_coleta_completa(self):
        # 1. Coleta fiel de cursos
        cursos = self.coletor.coletar_dados_completos()
        
        # 2. Extra√ß√£o detalhada de ofertas
        ofertas = self.extrator_ofertas.extrair_ofertas_detalhadas(cursos)
        
        # 3. An√°lise DEIA n√£o-destrutiva
        analise_deia = self.analisador_deia.processar_analise_deia(cursos)
        
        return cursos, ofertas, analise_deia
```

---

## üìã Conclus√£o

### **Resumo da Compara√ß√£o:**

1. **üîç Escopo**: Scraper original foca em ofertas detalhadas, sistema modular foca em integridade de dados
2. **üìä An√°lise**: Scraper original integra DEIA, sistema modular separa an√°lises
3. **üíæ Persist√™ncia**: Scraper original usa CSV incremental, sistema modular usa m√∫ltiplos formatos
4. **üìù Monitoramento**: Scraper original usa logs b√°sicos, sistema modular usa logging estruturado
5. **üèóÔ∏è Arquitetura**: Scraper original √© monol√≠tico, sistema modular √© separado

### **Recomenda√ß√£o Final:**
- **Para pesquisa acad√™mica rigorosa**: Use o sistema modular
- **Para an√°lise r√°pida com ofertas**: Use o scraper original
- **Para desenvolvimento futuro**: Considere integrar as melhores caracter√≠sticas de ambos

**Ambos os sistemas s√£o v√°lidos e complementares, servindo a diferentes prop√≥sitos de pesquisa e an√°lise.** üéØ 