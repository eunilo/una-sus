# ğŸ” AnÃ¡lise Comparativa: Scraper Original vs. Modelo Modular

## ğŸ“‹ VisÃ£o Geral da ComparaÃ§Ã£o

Esta anÃ¡lise compara o **`scraper_unasus.py`** (versÃ£o original robusta) com o **sistema modular de coleta** implementado na pasta `Grounded Theory`, especificamente o **`coletor_unasus_completo.py`**.

---

## ğŸ¯ Objetivos da ComparaÃ§Ã£o

### **Scraper Original (`scraper_unasus.py`)**
- âœ… **Coleta bÃ¡sica** de dados UNA-SUS
- âœ… **AnÃ¡lise DEIA integrada** durante a coleta
- âœ… **ExtraÃ§Ã£o de ofertas** detalhadas
- âœ… **Salvamento incremental** em CSV
- âœ… **Controle de duplicatas** por ID

### **Sistema Modular (`coletor_unasus_completo.py`)**
- âœ… **Coleta completa** sem filtros
- âœ… **SeparaÃ§Ã£o** entre coleta e anÃ¡lise
- âœ… **Database fiel** preservado
- âœ… **Sistema robusto** de logging
- âœ… **Checkpointing avanÃ§ado**

---

## ğŸ—ï¸ Arquitetura Comparativa

### **1. Estrutura de CÃ³digo**

#### **A. Scraper Original (MonolÃ­tico)**
```python
# Estrutura monolÃ­tica - tudo em um arquivo
â”œâ”€â”€ ConfiguraÃ§Ãµes (URL, headers, cookies, payload)
â”œâ”€â”€ FunÃ§Ãµes de anÃ¡lise DEIA
â”œâ”€â”€ FunÃ§Ãµes de extraÃ§Ã£o de ofertas
â”œâ”€â”€ Loop principal de coleta
â””â”€â”€ Salvamento incremental
```

**CaracterÃ­sticas:**
- ğŸ“ **1 arquivo** com todas as funcionalidades
- ğŸ”„ **Coleta + AnÃ¡lise** integradas
- ğŸ“Š **Filtros DEIA** aplicados durante coleta
- ğŸ’¾ **Salvamento direto** em CSV

#### **B. Sistema Modular (Separado)**
```python
# Estrutura modular - separaÃ§Ã£o de responsabilidades
â”œâ”€â”€ ColetorUnasusCompleto (coleta pura)
â”œâ”€â”€ ProcessadorDEIA (anÃ¡lise separada)
â”œâ”€â”€ AnalisadorGeral (anÃ¡lises flexÃ­veis)
â””â”€â”€ Orquestrador (coordenaÃ§Ã£o)
```

**CaracterÃ­sticas:**
- ğŸ“ **MÃºltiplos mÃ³dulos** especializados
- ğŸ”„ **Coleta independente** da anÃ¡lise
- ğŸ“Š **AnÃ¡lises nÃ£o-destrutivas**
- ğŸ’¾ **MÃºltiplos formatos** de saÃ­da

---

## ğŸ”§ ComparaÃ§Ã£o TÃ©cnica Detalhada

### **1. ConfiguraÃ§Ãµes de Rede**

#### **A. Scraper Original**
```python
# ConfiguraÃ§Ãµes bÃ¡sicas
url = "https://www.unasus.gov.br/cursos/rest/busca"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "X-Requested-With": "XMLHttpRequest",
    "Origin": "https://www.unasus.gov.br",
    "Referer": "https://www.unasus.gov.br/cursos/busca?status=todos&busca=&ordenacao=Relev%C3%A2ncia%20na%20busca"
}

cookies = {
    "PORTAL_UNASUS": "4ru34cs848mfbopb6vseqluni4",
    "UNASUSAnonID": "ID1ef7d6246158f7cf31c06b928bc56f8e",
    "_shibsession_64656661756c7468747470733a2f2f7777772e756e617375732e676f762e6272": "_329a72cffc11d2904ae393c82d0cfb72"
}

payload = {"busca": "", "ordenacao": "Por nome", "status": "Todos", "proximo": 0}
```

#### **B. Sistema Modular**
```python
# ConfiguraÃ§Ãµes idÃªnticas (baseadas no backup original)
self.url_base = "https://www.unasus.gov.br/cursos/rest/busca"
self.headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "X-Requested-With": "XMLHttpRequest",
    "Origin": "https://www.unasus.gov.br",
    "Referer": "https://www.unasus.gov.br/cursos/busca?status=todos&busca=&ordenacao=Relev%C3%A2ncia%20na%20busca"
}

self.cookies = {
    "PORTAL_UNASUS": "4ru34cs848mfbopb6vseqluni4",
    "UNASUSAnonID": "ID1ef7d6246158f7cf31c06b928bc56f8e",
    "_shibsession_64656661756c7468747470733a2f2f7777772e756e617375732e676f762e6272": "_329a72cffc11d2904ae393c82d0cfb72"
}

self.payload = {
    "busca": "",
    "ordenacao": "Por nome", 
    "status": "Todos",
    "proximo": 0
}
```

**âœ… ConclusÃ£o**: ConfiguraÃ§Ãµes **idÃªnticas** - ambos usam o mesmo modelo de requisiÃ§Ã£o.

### **2. Descritores DEIA**

#### **A. Scraper Original (Limitado)**
```python
descritores = [
    "Diversidade, Equidade e IntegraÃ§Ã£o",
    "Diversidade, Equidade, InclusÃ£o e Pertencimento",
    "Diversidade, Equidade, InclusÃ£o, Acessibilidade",
    "Diversidade, Igualdade e InclusÃ£o",
    "Diversidade, Igualdade, InclusÃ£o e Acessibilidade",
    "Diversidade, Igualdade, InclusÃ£o, Pertencimento",
    "Equidade, Diversidade e InclusÃ£o",
    "InclusÃ£o, Diversidade, Equidade e Acessibilidade",
    "InclusÃ£o, Diversidade, Equidade, Acessibilidade"
]
```

**CaracterÃ­sticas:**
- ğŸ“ **10 descritores** especÃ­ficos
- ğŸ¯ **Foco em frases completas**
- ğŸ” **Aplicado durante coleta**

#### **B. Sistema Modular (Expandido)**
```python
DESCRITORES_DEIA = {
    "diversidade": [
        "diversidade", "diverso", "pluralidade", "multicultural",
        "intercultural", "transcultural", "multirracial", "heterogeneidade"
    ],
    "equidade": [
        "equidade", "equitativo", "justiÃ§a social", "igualdade",
        "paridade", "equilibrio", "redistribuiÃ§Ã£o", "justiÃ§a"
    ],
    "inclusÃ£o": [
        "inclusÃ£o", "inclusivo", "acolhimento", "integraÃ§Ã£o",
        "participaÃ§Ã£o", "pertencimento", "comunidade", "aceitaÃ§Ã£o"
    ],
    "acessibilidade": [
        "acessibilidade", "acessÃ­vel", "acesso", "barreiras",
        "adaptaÃ§Ã£o", "tecnologia assistiva", "design universal"
    ],
    "populaÃ§Ãµes_especÃ­ficas": [
        "populaÃ§Ã£o negra", "indÃ­gena", "quilombola", "ribeirinha",
        "pessoas com deficiÃªncia", "LGBTQIA+", "mulheres", "idosos"
    ]
}
```

**CaracterÃ­sticas:**
- ğŸ“ **50+ descritores** organizados por categoria
- ğŸ¯ **Foco em palavras-chave**
- ğŸ” **Aplicado apÃ³s coleta** (nÃ£o-destrutivo)

### **3. Processamento de Dados**

#### **A. Scraper Original**
```python
def encontrar_descritor(titulo, descricao, descritores):
    """Encontra descritores DEIA no tÃ­tulo e descriÃ§Ã£o do curso."""
    texto = (titulo or "") + " " + (descricao or "")
    for descritor in descritores:
        if descritor.lower() in texto.lower():
            return descritor
    return ""

# AplicaÃ§Ã£o durante coleta
encontrado = encontrar_descritor(titulo, descricao, descritores)
curso["tem_deia"] = "Sim" if encontrado else "NÃ£o"
curso["deia_encontrado"] = encontrado
```

**CaracterÃ­sticas:**
- ğŸ” **AnÃ¡lise simples** (busca exata)
- ğŸ“Š **Integrada** na coleta
- ğŸ¯ **Apenas tÃ­tulo e descriÃ§Ã£o**

#### **B. Sistema Modular**
```python
def _identificar_elementos_deia(self, texto: str) -> List[Dict]:
    """Identifica elementos DEIA com contexto."""
    elementos_encontrados = []
    
    for categoria, descritores in self.descritores_deia.items():
        for descritor in descritores:
            if descritor.lower() in texto.lower():
                # Encontrar contexto
                contexto = self._extrair_contexto(texto, descritor)
                elementos_encontrados.append({
                    "categoria": categoria,
                    "descritor": descritor,
                    "contexto": contexto,
                    "campo": campo_origem
                })
    
    return elementos_encontrados
```

**CaracterÃ­sticas:**
- ğŸ” **AnÃ¡lise avanÃ§ada** com contexto
- ğŸ“Š **Separada** da coleta
- ğŸ¯ **MÃºltiplos campos** analisados

### **4. ExtraÃ§Ã£o de Ofertas**

#### **A. Scraper Original (Detalhada)**
```python
def extrair_ofertas_do_curso(id_curso):
    """Extrai ofertas de um curso especÃ­fico com logs detalhados."""
    url_curso = f"https://www.unasus.gov.br/cursos/curso/{id_curso}"
    
    # Busca ofertas ativas
    for link in soup.find_all("a", href=True):
        if any(pattern in href for pattern in ["/cursos/oferta/", "../oferta/", "oferta/"]):
            id_oferta = href.split("/")[-1]
            if id_oferta.isdigit():
                ofertas.append(id_oferta)
    
    # Busca ofertas encerradas
    botoes_encerradas = soup.find_all("a", string=lambda t: t and "encerrada" in t.lower())
    
    return ofertas

def extrair_dados_oferta(id_oferta):
    """Extrai dados detalhados de uma oferta."""
    # ExtraÃ§Ã£o de 20+ campos especÃ­ficos
    dados = {
        "id_oferta": id_oferta,
        "titulo_oferta": "",
        "carga_horaria": "",
        "vagas": "",
        "inscricoes_abertas": "",
        "data_inicio": "",
        "data_fim": "",
        "coordenador": "",
        "tutores": "",
        "certificacao": "",
        "pre_requisitos": "",
        "objetivos": "",
        "metodologia": "",
        "avaliacao": "",
        "bibliografia": "",
        "temas": "",
        "decs": "",
        "descricao_oferta": "",
        "palavras_chave": ""
    }
```

**CaracterÃ­sticas:**
- ğŸ” **ExtraÃ§Ã£o detalhada** de ofertas
- ğŸ“Š **20+ campos** especÃ­ficos
- ğŸ¯ **Foco em ofertas** individuais

#### **B. Sistema Modular (Simplificada)**
```python
def _processar_curso_completo(self, curso: Dict) -> Dict:
    """Processa um curso mantendo TODOS os dados originais."""
    curso_processado = curso.copy()
    
    # Adicionar metadados de coleta
    curso_processado["metadata_coleta"] = {
        "timestamp_coleta": datetime.now().isoformat(),
        "pagina_coleta": self.pagina_atual,
        "versao_coletor": "1.0.0",
        "tipo_coleta": "completa_sem_filtros"
    }
    
    # Garantir campos obrigatÃ³rios
    campos_obrigatorios = [
        "id", "titulo", "descricao", "carga_horaria", "status",
        "categoria", "publico_alvo", "palavras_chave", "link",
        "vagas", "numero_vagas", "qt_vagas", "vagas_disponiveis",
        "inicio_inscricao", "fim_inscricao", "data_inicio", "data_fim",
        "modalidade", "tipo_curso", "nivel", "area_tematica",
        "instituicao", "coordenador", "tutores", "certificacao",
        "pre_requisitos", "objetivos", "metodologia", "avaliacao",
        "bibliografia"
    ]
```

**CaracterÃ­sticas:**
- ğŸ” **PreservaÃ§Ã£o completa** dos dados originais
- ğŸ“Š **Metadados** de coleta
- ğŸ¯ **Foco na integridade** dos dados

### **5. Salvamento de Dados**

#### **A. Scraper Original**
```python
# Salvamento incremental
csv_path = "unasus_ofertas_detalhadas.csv"
lote = 10  # Salva a cada 10 cursos

if len(todos_detalhes) >= lote:
    if os.path.exists(csv_path):
        df_existente = pd.read_csv(csv_path, encoding="utf-8-sig")
        df_novo = pd.DataFrame(todos_detalhes)
        df_final = pd.concat([df_existente, df_novo], ignore_index=True)
    else:
        df_final = pd.DataFrame(todos_detalhes)
    df_final.to_csv(csv_path, index=False, encoding="utf-8-sig")
```

**CaracterÃ­sticas:**
- ğŸ’¾ **Apenas CSV**
- ğŸ”„ **Salvamento incremental**
- ğŸ“Š **Controle de duplicatas**

#### **B. Sistema Modular**
```python
def _salvar_dados_completos(self):
    """Salva dados em mÃºltiplos formatos."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Salvar em JSON
    json_path = f"dados/unasus_dados_completos_{timestamp}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(self.dados_coletados, f, ensure_ascii=False, indent=2)
    
    # Salvar em CSV
    csv_path = f"dados/unasus_dados_completos_{timestamp}.csv"
    df = pd.DataFrame(self.dados_coletados)
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    
    # Salvar em Excel (opcional)
    try:
        excel_path = f"dados/unasus_dados_completos_{timestamp}.xlsx"
        df.to_excel(excel_path, index=False)
    except ImportError:
        self.logger.info("openpyxl nÃ£o instalado. Pulando salvamento Excel.")
```

**CaracterÃ­sticas:**
- ğŸ’¾ **MÃºltiplos formatos** (JSON, CSV, Excel)
- ğŸ”„ **Checkpointing robusto**
- ğŸ“Š **Logs detalhados**

### **6. Logging e Monitoramento**

#### **A. Scraper Original**
```python
# Logs bÃ¡sicos com print
print(f"Buscando ofertas do curso {id_curso}...")
print(f"  âœ… Oferta encontrada: {id_oferta}")
print(f"Progresso salvo apÃ³s {len(cursos_processados)} cursos")
print(f"PÃ¡gina {pagina} processada.")
```

**CaracterÃ­sticas:**
- ğŸ“ **Logs bÃ¡sicos** com print
- ğŸ”„ **Sem persistÃªncia** de logs
- ğŸ“Š **InformaÃ§Ãµes limitadas**

#### **B. Sistema Modular**
```python
def _configurar_logger(self) -> logging.Logger:
    """Configura o logger para o coletor."""
    os.makedirs("logs", exist_ok=True)
    
    logger = logging.getLogger("ColetorUnasusCompleto")
    logger.setLevel(logging.INFO)
    
    # Handler para arquivo
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    fh = logging.FileHandler(f"logs/coletor_unasus_{timestamp}.log", encoding="utf-8")
    
    # Handler para console
    ch = logging.StreamHandler()
    
    # Formato
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger
```

**CaracterÃ­sticas:**
- ğŸ“ **Logs estruturados** com logging
- ğŸ”„ **PersistÃªncia** em arquivos
- ğŸ“Š **InformaÃ§Ãµes detalhadas**

---

## ğŸ“Š ComparaÃ§Ã£o de Funcionalidades

### **1. Escopo de Coleta**

| Aspecto | Scraper Original | Sistema Modular |
|---------|------------------|-----------------|
| **Dados Coletados** | Cursos + Ofertas | Apenas Cursos |
| **Campos ExtraÃ­dos** | 20+ campos especÃ­ficos | Todos os campos originais |
| **Integridade** | Processamento durante coleta | PreservaÃ§Ã£o completa |
| **Filtros** | DEIA aplicado durante coleta | Sem filtros na coleta |

### **2. AnÃ¡lise DEIA**

| Aspecto | Scraper Original | Sistema Modular |
|---------|------------------|-----------------|
| **Descritores** | 10 frases especÃ­ficas | 50+ palavras-chave |
| **AplicaÃ§Ã£o** | Durante coleta | ApÃ³s coleta |
| **Campos Analisados** | TÃ­tulo + DescriÃ§Ã£o | MÃºltiplos campos |
| **Contexto** | NÃ£o | Sim |
| **CategorizaÃ§Ã£o** | Sim/NÃ£o | Categorias detalhadas |

### **3. PersistÃªncia de Dados**

| Aspecto | Scraper Original | Sistema Modular |
|---------|------------------|-----------------|
| **Formatos** | CSV apenas | JSON, CSV, Excel |
| **Checkpointing** | BÃ¡sico | Robusto |
| **Controle de Duplicatas** | Por ID de curso | Por ID de curso |
| **Metadados** | NÃ£o | Sim |

### **4. Monitoramento**

| Aspecto | Scraper Original | Sistema Modular |
|---------|------------------|-----------------|
| **Logs** | Print bÃ¡sico | Logging estruturado |
| **PersistÃªncia** | NÃ£o | Sim |
| **NÃ­veis** | Apenas info | Debug, Info, Warning, Error |
| **RelatÃ³rios** | NÃ£o | Sim |

---

## ğŸ¯ Vantagens e Desvantagens

### **Scraper Original**

#### **âœ… Vantagens:**
- ğŸš€ **Simplicidade**: CÃ³digo monolÃ­tico fÃ¡cil de entender
- ğŸ” **Detalhamento**: ExtraÃ§Ã£o detalhada de ofertas
- ğŸ“Š **AnÃ¡lise integrada**: DEIA aplicado durante coleta
- ğŸ’¾ **Salvamento direto**: CSV pronto para uso

#### **âŒ Desvantagens:**
- ğŸ”„ **Acoplamento**: Coleta e anÃ¡lise misturadas
- ğŸ“Š **Filtros**: Dados filtrados durante coleta
- ğŸ’¾ **Formato Ãºnico**: Apenas CSV
- ğŸ“ **Logs limitados**: Sem persistÃªncia

### **Sistema Modular**

#### **âœ… Vantagens:**
- ğŸ§© **Modularidade**: SeparaÃ§Ã£o clara de responsabilidades
- ğŸ“Š **Integridade**: Database fiel preservado
- ğŸ” **Flexibilidade**: AnÃ¡lises nÃ£o-destrutivas
- ğŸ’¾ **MÃºltiplos formatos**: JSON, CSV, Excel
- ğŸ“ **Logs robustos**: PersistÃªncia e estruturaÃ§Ã£o
- ğŸ”„ **Checkpointing**: RecuperaÃ§Ã£o de falhas

#### **âŒ Desvantagens:**
- ğŸ—ï¸ **Complexidade**: MÃºltiplos mÃ³dulos
- ğŸ” **Escopo limitado**: Sem extraÃ§Ã£o de ofertas
- ğŸ“š **Curva de aprendizado**: Mais arquivos para entender

---

## ğŸ”„ Fluxos de Trabalho Comparativos

### **1. Scraper Original**
```
ğŸŒ ConexÃ£o UNA-SUS
    â†“
ğŸ“„ PaginaÃ§Ã£o automÃ¡tica
    â†“
ğŸ” ExtraÃ§Ã£o de cursos
    â†“
ğŸ“Š AnÃ¡lise DEIA (durante coleta)
    â†“
ğŸ” ExtraÃ§Ã£o de ofertas
    â†“
ğŸ’¾ Salvamento CSV incremental
```

### **2. Sistema Modular**
```
ğŸŒ ConexÃ£o UNA-SUS
    â†“
ğŸ“„ PaginaÃ§Ã£o automÃ¡tica
    â†“
ğŸ” Coleta completa (sem filtros)
    â†“
ğŸ’¾ Salvamento mÃºltiplos formatos
    â†“
ğŸ“Š AnÃ¡lise DEIA (separada)
    â†“
ğŸ“ˆ AnÃ¡lise geral (separada)
```

---

## ğŸ¯ RecomendaÃ§Ãµes de Uso

### **Use Scraper Original quando:**
- ğŸ¯ **Precisa de ofertas detalhadas**
- ğŸš€ **Quer simplicidade**
- ğŸ“Š **AnÃ¡lise DEIA bÃ¡sica Ã© suficiente**
- ğŸ’¾ **CSV Ã© o formato desejado**

### **Use Sistema Modular quando:**
- ğŸ¯ **Precisa de database fiel**
- ğŸ” **Quer anÃ¡lises flexÃ­veis**
- ğŸ“Š **Pesquisa acadÃªmica rigorosa**
- ğŸ’¾ **MÃºltiplos formatos de saÃ­da**
- ğŸ“ **Logs detalhados necessÃ¡rios**

---

## ğŸ”® EvoluÃ§Ã£o e IntegraÃ§Ã£o

### **PossÃ­vel IntegraÃ§Ã£o:**
```python
# Combinar o melhor dos dois mundos
class ScraperUnasusIntegrado:
    def __init__(self):
        self.coletor = ColetorUnasusCompleto()  # Coleta fiel
        self.extrator_ofertas = ExtratorOfertas()  # ExtraÃ§Ã£o detalhada
        self.analisador_deia = ProcessadorDEIA()  # AnÃ¡lise avanÃ§ada
    
    def executar_coleta_completa(self):
        # 1. Coleta fiel de cursos
        cursos = self.coletor.coletar_dados_completos()
        
        # 2. ExtraÃ§Ã£o detalhada de ofertas
        ofertas = self.extrator_ofertas.extrair_ofertas_detalhadas(cursos)
        
        # 3. AnÃ¡lise DEIA nÃ£o-destrutiva
        analise_deia = self.analisador_deia.processar_analise_deia(cursos)
        
        return cursos, ofertas, analise_deia
```

---

## ğŸ“‹ ConclusÃ£o

### **Resumo da ComparaÃ§Ã£o:**

1. **ğŸ” Escopo**: Scraper original foca em ofertas detalhadas, sistema modular foca em integridade de dados
2. **ğŸ“Š AnÃ¡lise**: Scraper original integra DEIA, sistema modular separa anÃ¡lises
3. **ğŸ’¾ PersistÃªncia**: Scraper original usa CSV incremental, sistema modular usa mÃºltiplos formatos
4. **ğŸ“ Monitoramento**: Scraper original usa logs bÃ¡sicos, sistema modular usa logging estruturado
5. **ğŸ—ï¸ Arquitetura**: Scraper original Ã© monolÃ­tico, sistema modular Ã© separado

### **RecomendaÃ§Ã£o Final:**
- **Para pesquisa acadÃªmica rigorosa**: Use o sistema modular
- **Para anÃ¡lise rÃ¡pida com ofertas**: Use o scraper original
- **Para desenvolvimento futuro**: Considere integrar as melhores caracterÃ­sticas de ambos

**Ambos os sistemas sÃ£o vÃ¡lidos e complementares, servindo a diferentes propÃ³sitos de pesquisa e anÃ¡lise.** ğŸ¯ 